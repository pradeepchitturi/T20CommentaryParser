{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58524df-74cc-4bd5-aae7-4036bcceb08f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# libraries installation for serverless compute\n",
    "%pip install geopy timezonefinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4a3c6e-cab1-4104-a235-2f645eaad8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cricinfo Commentary Parser - Silver Layer (Incremental Load via Delta CDF)\n",
    "\n",
    "**Bronze Layer Tables (CDF Enabled):**\n",
    "- `T20_catalog.bronze.match_events` (ball-by-ball CSV)\n",
    "- `T20_catalog.bronze.match_metadata` (match-level JSON)\n",
    "- `T20_catalog.bronze.match_players` (player-level CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c322e5ca-f57e-4995-bad8-d3f77421a8f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b2c56e-50c4-4a15-ac15-554a9a37d6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Job Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Default values are used during interactive runs.\n",
    "# Databricks Job overrides these at runtime via the Parameters section.\n",
    "# Key names here must match exactly what the Job defines.\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"catalog_name\", \"T20_catalog\",\n",
    "    \"Catalog Name\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c650046d-ad7b-4906-9318-d1ce80fe670c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, regexp_extract, regexp_replace, trim, first, create_map,\n",
    "    coalesce, collect_set, lit, current_timestamp, split, size,\n",
    "    sum as spark_sum, udf, to_timestamp, concat, to_utc_timestamp,\n",
    "    to_date, try_element_at, monotonically_increasing_id, row_number,element_at,\n",
    "    desc, asc\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from geopy.geocoders import Nominatim\n",
    "from timezonefinder import TimezoneFinder\n",
    "from pyspark.sql import Row\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "CATALOG  = dbutils.widgets.get(\"catalog_name\")\n",
    "BRONZE = f\"{CATALOG}.bronze\"\n",
    "SILVER = f\"{CATALOG}.silver\"\n",
    "\n",
    "# Table names\n",
    "BRONZE_EVENTS   = f\"{BRONZE}.match_events\"\n",
    "BRONZE_PLAYERS  = f\"{BRONZE}.match_players\"\n",
    "BRONZE_METADATA = f\"{BRONZE}.match_metadata\"\n",
    "SILVER_EVENTS   = f\"{SILVER}.match_events\"\n",
    "SILVER_PLAYERS  = f\"{SILVER}.match_players\"\n",
    "SILVER_METADATA = f\"{SILVER}.match_metadata\"\n",
    "TIMEZONE_CACHE_TABLE = f\"{SILVER}.ground_timezone_map\"\n",
    "CDF_CHECKPOINT_TABLE = f\"{SILVER}.cdf_checkpoint\"\n",
    "\n",
    "# CDF metadata columns to drop after filtering\n",
    "CDF_META_COLS = [\"_change_type\", \"_commit_version\", \"_commit_timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd1c3529-b7cc-4c5f-bdac-a14e5553f9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Checkpoint & Silver Schema (One-time Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "637eb701-2c6f-488d-836c-f36f6114c09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create silver schema if not exists\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SILVER}\")\n",
    "\n",
    "# Checkpoint table tracks the last processed Delta version per source table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CDF_CHECKPOINT_TABLE} (\n",
    "        source_table STRING,\n",
    "        last_processed_version LONG,\n",
    "        last_processed_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d19bdf55-80dd-4637-8c7b-5ece38df4092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CDF Helper Functions & Utility Functions\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `read_cdf_changes()` | Read CDF from _delta_log since last checkpoint |\n",
    "| `get_affected_match_ids()` | Extract matchids from CDF insert/update rows |\n",
    "| `dedup_dataframe()` | Drop duplicates excluding metadata columns |\n",
    "| `dedup_column_values()` | Remove repetitive text within column values |\n",
    "| `merge_to_silver()` | Unified merge/create logic for any Silver table |\n",
    "| `propagate_deletes()` | Unified CDF delete propagation |\n",
    "| `build_ground_timezone_map()` | Geocode grounds â†’ timezone with Delta cache |\n",
    "| `local_time_to_utc()` | Convert local match times to UTC |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae9b97a-9d47-48ca-9153-6682065317cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CDF Functions\n",
    "# ============================================================\n",
    "\n",
    "def get_last_processed_version(table_name):\n",
    "    \"\"\"Get the last successfully processed CDF version for a source table.\"\"\"\n",
    "    try:\n",
    "        result = spark.sql(f\"\"\"\n",
    "            SELECT last_processed_version \n",
    "            FROM {CDF_CHECKPOINT_TABLE}\n",
    "            WHERE source_table = '{table_name}'\n",
    "        \"\"\").collect()\n",
    "        return result[0][0] if result else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_current_table_version(table_name):\n",
    "    \"\"\"Get the latest Delta version of a table.\"\"\"\n",
    "    return spark.sql(f\"DESCRIBE HISTORY {table_name} LIMIT 1\").collect()[0][\"version\"]\n",
    "\n",
    "\n",
    "def get_cdf_enabled_version(table_name):\n",
    "    \"\"\"Find the Delta version when CDF was first enabled.\"\"\"\n",
    "    history = spark.sql(f\"DESCRIBE HISTORY {table_name}\").collect()\n",
    "    for row in reversed(history):\n",
    "        ops = row[\"operationParameters\"] or {}\n",
    "        if \"delta.enableChangeDataFeed\" in str(ops):\n",
    "            return row[\"version\"]\n",
    "    return get_current_table_version(table_name)\n",
    "\n",
    "\n",
    "def update_checkpoint(table_name, version):\n",
    "    \"\"\"Update the CDF checkpoint with the latest processed version.\"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CDF_CHECKPOINT_TABLE} AS target\n",
    "        USING (\n",
    "            SELECT '{table_name}' AS source_table,\n",
    "                   {version} AS last_processed_version,\n",
    "                   current_timestamp() AS last_processed_timestamp\n",
    "        ) AS source\n",
    "        ON target.source_table = source.source_table\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def read_cdf_changes(table_name):\n",
    "    \"\"\"Read file-level changes from the Delta transaction log since last checkpoint.\"\"\"\n",
    "    current_version = get_current_table_version(table_name)\n",
    "    #print(current_version)\n",
    "    silver_exists = spark.catalog.tableExists(SILVER_EVENTS)\n",
    "    #print(silver_exists)\n",
    "    if not silver_exists:\n",
    "        print(f\"  ðŸ“¥ {table_name}: First run â†’ full table read\")\n",
    "        return spark.read.table(table_name), current_version\n",
    "\n",
    "    last_version = get_last_processed_version(table_name)\n",
    "\n",
    "    if last_version is not None and last_version >= current_version:\n",
    "        print(f\"  â­ï¸  {table_name}: No new commits (at version {current_version})\")\n",
    "        return None, current_version\n",
    "\n",
    "    start_version = (last_version + 1) if last_version is not None else get_cdf_enabled_version(table_name)\n",
    "\n",
    "    if start_version > current_version:\n",
    "        print(f\"  â­ï¸  {table_name}: No CDF changes after version {start_version}\")\n",
    "        return None, current_version\n",
    "\n",
    "    print(f\"  ðŸ“¥ {table_name}: CDF version {start_version} â†’ {current_version}\")\n",
    "    return (\n",
    "        spark.read.format(\"delta\")\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .option(\"startingVersion\", start_version)\n",
    "        .option(\"endingVersion\", current_version)\n",
    "        .table(table_name)\n",
    "    ), current_version\n",
    "\n",
    "\n",
    "def get_affected_match_ids(df_cdf):\n",
    "    \"\"\"Extract distinct matchid values from CDF or full-read DataFrames.\"\"\"\n",
    "    if df_cdf is None:\n",
    "        return set()\n",
    "    \n",
    "    # First run: full table read has no _change_type column\n",
    "    if \"_change_type\" in df_cdf.columns:\n",
    "        df_filtered = df_cdf.filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "    else:\n",
    "        df_filtered = df_cdf  # Full read â€” all rows are \"new\"\n",
    "    \n",
    "    return set(\n",
    "        row.matchid for row in\n",
    "        df_filtered.select(\"matchid\").distinct().collect()\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Merge & Delete Helpers (eliminates repeated code)\n",
    "# ============================================================\n",
    "\n",
    "def merge_to_silver(df_source, silver_table_name, merge_keys):\n",
    "    \"\"\"Unified merge/create for any Silver table.\n",
    "    \n",
    "    Args:\n",
    "        df_source: DataFrame to merge\n",
    "        silver_table_name: Full Silver table name\n",
    "        merge_keys: List of columns for merge condition\n",
    "    \"\"\"\n",
    "    table_exists = spark.catalog.tableExists(silver_table_name)\n",
    "\n",
    "    if not table_exists:\n",
    "        print(f\"  ðŸ†• Creating {silver_table_name} (first run)...\")\n",
    "        (\n",
    "            df_source.write.format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .saveAsTable(silver_table_name)\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  ðŸ”„ Merging into {silver_table_name}...\")\n",
    "        condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in merge_keys])\n",
    "        (\n",
    "            DeltaTable.forName(spark, silver_table_name).alias(\"target\")\n",
    "            .merge(df_source.alias(\"source\"), condition)\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    print(f\"  âœ… {silver_table_name} complete\")\n",
    "    return table_exists\n",
    "\n",
    "\n",
    "def propagate_deletes(df_cdf, silver_table_name, merge_keys, table_existed):\n",
    "    \"\"\"Unified CDF delete propagation for any Silver table.\"\"\"\n",
    "    if df_cdf is None or not table_existed:\n",
    "        return\n",
    "    df_deletes = df_cdf.filter(col(\"_change_type\") == \"delete\")\n",
    "    # Cache and count once\n",
    "    #df_deletes.cache()\n",
    "    df_deletes.createOrReplaceTempView(\"deletes_temp\")\n",
    "    df_deletes = spark.table(\"deletes_temp\")\n",
    "\n",
    "    delete_count = df_deletes.count()\n",
    "    if delete_count > 0:\n",
    "        print(f\"  ðŸ—‘ï¸  Propagating {delete_count} deletes to {silver_table_name}...\")\n",
    "        condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in merge_keys])\n",
    "        (\n",
    "            DeltaTable.forName(spark, silver_table_name).alias(\"target\")\n",
    "            .merge(df_deletes.alias(\"source\"), condition)\n",
    "            .whenMatchedDelete()\n",
    "            .execute()\n",
    "        )\n",
    "        print(f\"  âœ… {delete_count} records deleted\")\n",
    "    else:\n",
    "        print(f\"  ðŸ—‘ï¸  No CDF deletes for {silver_table_name}\")\n",
    "    #df_deletes.unpersist()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Data Quality / Dedup Functions\n",
    "# ============================================================\n",
    "\n",
    "@udf(StringType())\n",
    "def remove_duplicate_text(value):\n",
    "    \"\"\"Remove duplicate/repetitive entries from a comma-delimited string.\"\"\"\n",
    "    if not value:\n",
    "        return value\n",
    "    parts = [p.strip() for p in value.split(\" \")]  # FIX: split by comma, not space\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for part in parts:\n",
    "        if part and part.lower() not in seen:\n",
    "            seen.add(part.lower())\n",
    "            unique.append(part)\n",
    "    return \" \".join(unique)\n",
    "\n",
    "\n",
    "def dedup_column_values(df, columns):\n",
    "    \"\"\"Remove duplicate/repetitive text from specified columns.\"\"\"\n",
    "    valid_cols = [c for c in columns if c in df.columns]\n",
    "    skipped_cols = [c for c in columns if c not in df.columns]\n",
    "    for c in valid_cols:\n",
    "        df = df.withColumn(c, remove_duplicate_text(trim(col(c))))\n",
    "    print(f\"  ðŸ§¹ Deduped text in {len(valid_cols)} column(s): {', '.join(valid_cols)}\")\n",
    "    if skipped_cols:\n",
    "        print(f\"     Skipped (not found): {', '.join(skipped_cols)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def dedup_dataframe(df, exclude_cols=None, order_by_col=None, keep=\"latest\"):\n",
    "    \"\"\"Drop duplicates excluding metadata columns. Uses window-based dedup.\"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = [\"load_timestamp\", \"batch_id\", \"source_file\", \"_rescued_data\"]\n",
    "\n",
    "    exclude_cols = [c for c in exclude_cols if c in df.columns]\n",
    "    dedup_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "    if order_by_col and order_by_col in df.columns:\n",
    "        sort_order = desc(order_by_col) if keep == \"latest\" else asc(order_by_col)\n",
    "        window = Window.partitionBy(dedup_cols).orderBy(sort_order)\n",
    "        df_result = (\n",
    "            df.withColumn(\"_rank\", row_number().over(window))\n",
    "            .filter(col(\"_rank\") == 1)\n",
    "            .drop(\"_rank\")\n",
    "        )\n",
    "    else:\n",
    "        df_result = df.dropDuplicates(dedup_cols)\n",
    "\n",
    "    print(f\"  ðŸ”„ Dedup complete on {len(dedup_cols)} columns\")\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Timezone Functions\n",
    "# ============================================================\n",
    "\n",
    "def build_ground_timezone_map(df, ground_col=\"ground\"):\n",
    "    \"\"\"Geocode grounds â†’ timezone with Delta cache + city fallback.\"\"\"\n",
    "    cached_map = {}\n",
    "    cache_exists = spark.catalog.tableExists(TIMEZONE_CACHE_TABLE)\n",
    "\n",
    "    if cache_exists:\n",
    "        cached_map = {r.ground: r.local_timezone for r in spark.read.table(TIMEZONE_CACHE_TABLE).collect()}\n",
    "        print(f\"  ðŸ“¦ Cache: {len(cached_map)} ground(s)\")\n",
    "\n",
    "    all_grounds = [\n",
    "        r[ground_col] for r in\n",
    "        df.select(ground_col).distinct().filter(col(ground_col).isNotNull()).collect()\n",
    "    ]\n",
    "    new_grounds = [g for g in all_grounds if g not in cached_map]\n",
    "\n",
    "    if not new_grounds:\n",
    "        print(f\"  âœ… All {len(all_grounds)} ground(s) cached\")\n",
    "        return cached_map\n",
    "\n",
    "    print(f\"  ðŸŒ Geocoding {len(new_grounds)} new ground(s)...\")\n",
    "    geolocator = Nominatim(user_agent=\"cricket_data_pipeline\", timeout=10)\n",
    "    tf = TimezoneFinder()\n",
    "    new_entries = {}\n",
    "\n",
    "    def _geocode(query):\n",
    "        try:\n",
    "            loc = geolocator.geocode(query)\n",
    "            time.sleep(1.1)\n",
    "            if loc:\n",
    "                return tf.timezone_at(lat=loc.latitude, lng=loc.longitude)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    for ground in new_grounds:\n",
    "        short = ground.split(\",\")[0].strip()\n",
    "        city_parts = [p.strip() for p in ground.split(\",\")]\n",
    "        city = city_parts[-1] if len(city_parts) >= 2 else None\n",
    "\n",
    "        # Try: full name â†’ short + cricket stadium â†’ city\n",
    "        tz = _geocode(ground) or _geocode(short + \" cricket stadium\") or (city and _geocode(city))\n",
    "\n",
    "        new_entries[ground] = tz\n",
    "        status = f\"âœ… {tz}\" if tz else \"âŒ unresolved\"\n",
    "        print(f\"     {ground} â†’ {status}\")\n",
    "\n",
    "    # Update cache\n",
    "    valid_new = {k: v for k, v in new_entries.items() if v}\n",
    "    if valid_new:\n",
    "        df_new = spark.createDataFrame([Row(ground=k, local_timezone=v) for k, v in valid_new.items()])\n",
    "        if cache_exists:\n",
    "            DeltaTable.forName(spark, TIMEZONE_CACHE_TABLE).alias(\"t\").merge(\n",
    "                df_new.alias(\"s\"), \"t.ground = s.ground\"\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "            # âœ— BEFORE â€” saveAsTable fails on serverless\n",
    "            # df_new.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TIMEZONE_CACHE_TABLE)\n",
    "\n",
    "            # âœ“ AFTER\n",
    "            df_new.createOrReplaceTempView(\"_tz_cache_temp\")\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE OR REPLACE TABLE {TIMEZONE_CACHE_TABLE}\n",
    "                USING DELTA\n",
    "                AS SELECT * FROM _tz_cache_temp\n",
    "                \"\"\")\n",
    "        print(f\"  ðŸ’¾ Cache updated: +{len(valid_new)} ground(s)\")\n",
    "\n",
    "    unresolved = [k for k, v in new_entries.items() if not v]\n",
    "    if unresolved:\n",
    "        print(f\"  âš ï¸  Unresolved: {unresolved}\")\n",
    "\n",
    "    return {**cached_map, **valid_new}\n",
    "\n",
    "\n",
    "def apply_timezone_map(df, timezone_map, ground_col=\"ground\", output_col=\"local_timezone\"):\n",
    "    \"\"\"Apply ground â†’ timezone mapping to DataFrame â€” serverless compatible.\"\"\"\n",
    "    valid_map = {k: v for k, v in timezone_map.items() if v}\n",
    "    mapping_expr = create_map([lit(x) for x in chain(*valid_map.items())])\n",
    "    \n",
    "    # âœ— BEFORE â€” bracket notation fails on serverless\n",
    "    # return df.withColumn(output_col, mapping_expr[col(ground_col)])\n",
    "    \n",
    "    # âœ“ AFTER â€” element_at works everywhere\n",
    "    return df.withColumn(output_col, element_at(mapping_expr, col(ground_col)))\n",
    "\n",
    "def local_time_to_utc(df, time_col, date_col, tz_col, output_col):\n",
    "    \"\"\"Convert HH.MM local time to UTC timestamp.\"\"\"\n",
    "    return df.withColumn(output_col,\n",
    "        to_utc_timestamp(\n",
    "            to_timestamp(\n",
    "                concat(col(date_col).cast(\"string\"), lit(\" \"),\n",
    "                       regexp_replace(col(time_col), r\"\\.\", \":\")),\n",
    "                \"yyyy-MM-dd HH:mm\"\n",
    "            ), col(tz_col)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5519ff8c-fb3c-4c0d-8aff-0882089387f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Name Resolution UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbd1e26-eeb8-41fa-a204-1d17d2b12840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Known team abbreviations that don't follow standard patterns\n",
    "KNOWN_TEAM_ABBREVS = {\n",
    "    \"USA\": \"United States Of America\", \"NED\": \"Netherlands\", \"BHR\": \"Bahrain\",\n",
    "    \"NZ\": \"New Zealand\", \"WI\": \"West Indies\", \"SL\": \"Sri Lanka\",\n",
    "    \"PNG\": \"Papua New Guinea\", \"UAE\": \"United Arab Emirates\", \"HK\": \"Hong Kong\",\n",
    "    \"RSA\": \"South Africa\", \"SA\": \"South Africa\", \"ENG\": \"England\",\n",
    "    \"AUS\": \"Australia\", \"IND\": \"India\", \"PAK\": \"Pakistan\",\n",
    "    \"BAN\": \"Bangladesh\", \"AFG\": \"Afghanistan\", \"ZIM\": \"Zimbabwe\",\n",
    "    \"IRE\": \"Ireland\", \"SCO\": \"Scotland\", \"NAM\": \"Namibia\",\n",
    "    \"NEP\": \"Nepal\", \"OMA\": \"Oman\", \"CAN\": \"Canada\", \"UGA\": \"Uganda\",\n",
    "}\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def get_full_name(short_name, name_list):\n",
    "    \"\"\"Resolve abbreviated team/player name to full name using fuzzy matching.\"\"\"\n",
    "    if not short_name or not name_list:\n",
    "        return short_name\n",
    "\n",
    "    # Parse name_list\n",
    "    if isinstance(name_list, list):\n",
    "        names = name_list\n",
    "    elif isinstance(name_list, str):\n",
    "        try:\n",
    "            names = json.loads(name_list)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            names = [n.strip().strip('\"') for n in name_list.strip(\"[]\").split(\",\")]\n",
    "    else:\n",
    "        return short_name\n",
    "\n",
    "    if short_name in names:\n",
    "        return short_name\n",
    "    if \"Super Over\" in short_name:\n",
    "        return short_name\n",
    "\n",
    "    abbr = short_name.strip()\n",
    "    abbr_upper = abbr.upper()\n",
    "    abbr_parts = abbr.split()\n",
    "\n",
    "    # 1. Known abbreviations lookup\n",
    "    if abbr_upper in KNOWN_TEAM_ABBREVS:\n",
    "        known_full = KNOWN_TEAM_ABBREVS[abbr_upper]\n",
    "        for name in names:\n",
    "            if name.upper() == known_full.upper():\n",
    "                return name\n",
    "\n",
    "    for name in names:\n",
    "        name_parts = name.split()\n",
    "        name_upper = name.upper()\n",
    "\n",
    "        # 2. All-word initials (SA -> South Africa)\n",
    "        initials = \"\".join(w[0] for w in name_parts).upper()\n",
    "        if abbr_upper == initials:\n",
    "            return name\n",
    "\n",
    "        # 3. Initials skipping lowercase words (USA -> United States Of America)\n",
    "        initials_upper_only = \"\".join(w[0] for w in name_parts if w[0].isupper()).upper()\n",
    "        if abbr_upper == initials_upper_only and initials_upper_only != initials:\n",
    "            return name\n",
    "\n",
    "        # 4. Starts-with (AFG -> Afghanistan)\n",
    "        if len(abbr) >= 2 and len(abbr_parts) == 1 and name_upper.startswith(abbr_upper):\n",
    "            return name\n",
    "\n",
    "        # 5. Exact last name (Stubbs -> Tristan Stubbs)\n",
    "        if len(abbr_parts) == 1 and abbr_upper == name_parts[-1].upper():\n",
    "            return name\n",
    "\n",
    "        # 6. Multi-word surname tail (de Kock -> Quinton de Kock)\n",
    "        if len(abbr_parts) >= 2 and len(name_parts) > len(abbr_parts):\n",
    "            tail = \" \".join(name_parts[-len(abbr_parts):]).upper()\n",
    "            if abbr_upper == tail:\n",
    "                return name\n",
    "\n",
    "        # 7. Concatenated initials + surname (JN Loftie-Eaton -> Jan Nicol Loftie-Eaton)\n",
    "        if len(abbr_parts) >= 2:\n",
    "            first_part = abbr_parts[0].upper().rstrip(\".\")\n",
    "            abbr_surname = \" \".join(abbr_parts[1:]).upper()\n",
    "            if (first_part.isalpha()\n",
    "                and first_part.isupper()\n",
    "                and len(first_part) >= 2\n",
    "                and len(first_part) <= len(name_parts) - 1):\n",
    "                name_initials = \"\".join(name_parts[i][0].upper() for i in range(len(first_part)))\n",
    "                remaining = \" \".join(name_parts[len(first_part):]).upper()\n",
    "                if first_part == name_initials and abbr_surname == remaining:\n",
    "                    return name\n",
    "\n",
    "        # 8. Single initial + last name (M Nabi -> Mohammad Nabi)\n",
    "        if len(abbr_parts) >= 2 and len(name_parts) >= 2:\n",
    "            abbr_first = abbr_parts[0].upper().rstrip(\".\")\n",
    "            if (len(abbr_first) == 1\n",
    "                and abbr_first == name_parts[0][0].upper()\n",
    "                and abbr_parts[-1].upper() == name_parts[-1].upper()):\n",
    "                return name\n",
    "\n",
    "        # 9. Abbreviated first + full last (R Khan -> Rashid Khan)\n",
    "        if len(abbr_parts) >= 2 and len(name_parts) >= 2:\n",
    "            abbr_last = \" \".join(abbr_parts[1:]).upper()\n",
    "            name_last = \" \".join(name_parts[1:]).upper()\n",
    "            abbr_f = abbr_parts[0].upper().rstrip(\".\")\n",
    "            if abbr_last == name_last and name_parts[0].upper().startswith(abbr_f):\n",
    "                return name\n",
    "\n",
    "        # 10. Partial first + matching last name\n",
    "        if len(abbr_parts) >= 2 and len(name_parts) >= 2:\n",
    "            if (abbr_parts[-1].upper() == name_parts[-1].upper()\n",
    "                and name_parts[0].upper().startswith(abbr_parts[0].upper())):\n",
    "                return name\n",
    "\n",
    "    return short_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49047c9f-46ca-47e4-8e19-18b935396357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read CDF Changes from Bronze Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f9343b-f83f-43ef-9824-5e5fddf9faa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Reading Change Data Feed from Bronze tables...\\n\")\n",
    "\n",
    "df_events_cdf, events_version     = read_cdf_changes(BRONZE_EVENTS)\n",
    "df_players_cdf, players_version   = read_cdf_changes(BRONZE_PLAYERS)\n",
    "df_metadata_cdf, metadata_version = read_cdf_changes(BRONZE_METADATA)\n",
    "#df_events_cdf.display()\n",
    "#print(events_version)\n",
    "\n",
    "if df_events_cdf is None and df_players_cdf is None and df_metadata_cdf is None:\n",
    "    print(\"\\nâœ… No new commits in Bronze. Silver layer is up to date.\")\n",
    "    dbutils.notebook.exit(\"NO_CHANGES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2ef0cd3-e48c-4502-9af7-bb5ebfbe6205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Collect Affected Match IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abf66bf-7de9-43d3-8a77-77eefb447a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event_match_ids    = get_affected_match_ids(df_events_cdf)\n",
    "player_match_ids   = get_affected_match_ids(df_players_cdf)\n",
    "metadata_match_ids = get_affected_match_ids(df_metadata_cdf)\n",
    "\n",
    "affected_match_ids = list(event_match_ids | player_match_ids | metadata_match_ids)\n",
    "print(affected_match_ids)\n",
    "\n",
    "print(f\"ðŸ“Š Affected matches: Events={len(event_match_ids)}, \"\n",
    "      f\"Players={len(player_match_ids)}, Metadata={len(metadata_match_ids)}, \"\n",
    "      f\"Total={len(affected_match_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fac2dad6-2daa-4039-9dc9-3d8c6ab342cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data for Affected Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6976c30-387d-4fb4-bbf5-4ee056719247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Events: CDF if available, else full read for player/metadata-only changes\n",
    "if df_events_cdf is not None:\n",
    "    if \"_change_type\" in df_events_cdf.columns:\n",
    "        df_events_to_process = (\n",
    "            df_events_cdf\n",
    "            .filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "            .drop(*CDF_META_COLS)\n",
    "        )\n",
    "    else:\n",
    "        # First run: full read, no CDF columns to drop\n",
    "        df_events_to_process = df_events_cdf\n",
    "else:\n",
    "    df_events_to_process = (\n",
    "        spark.read.table(BRONZE_EVENTS)\n",
    "        .filter(col(\"matchid\").isin(affected_match_ids))\n",
    "    )\n",
    "\n",
    "# Players & Metadata: always full read for affected matches\n",
    "df_players_full = spark.read.table(BRONZE_PLAYERS).filter(col(\"matchid\").isin(affected_match_ids))\n",
    "df_metadata_full = spark.read.table(BRONZE_METADATA).filter(col(\"matchid\").isin(affected_match_ids))\n",
    "# Build player name lookup once (reused by events & metadata)\n",
    "df_match_names = (\n",
    "    df_players_full.groupBy(\"matchid\")\n",
    "    .agg(\n",
    "        collect_set(\"team\").alias(\"team_names\"),\n",
    "        collect_set(\"player_name\").alias(\"player_names\")\n",
    "    )\n",
    ")\n",
    "df_match_names.createOrReplaceTempView(\"match_names_temp\")\n",
    "df_match_names = spark.table(\"match_names_temp\")\n",
    "\n",
    "print(f\"ðŸ“‹ Loaded: Events={df_events_to_process.count()}, \"\n",
    "      f\"Players={df_players_full.count()}, Metadata={df_metadata_full.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f459ce28-8602-49cc-b363-26c7192e2639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Applying Transformations on Events Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ef2adf-4d2d-46da-b18e-0f88f1a1a006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parse Event Column, Calculate Runs, Extract Overs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08420157-b1e4-420b-8a93-028ac2d56952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dedup events\n",
    "df_events = dedup_dataframe(df_events_to_process, order_by_col=\"load_timestamp\", keep=\"latest\")\n",
    "\n",
    "# Parse: \"Bowler to Batsman, Score\"\n",
    "EVENT_PATTERN = r\"([A-Za-z\\s\\'\\-]*)\\sto\\s([A-Za-z\\s\\'\\-]*)\\,\\s+([0-9A-Za-z\\,\\s\\(\\)]*)\"\n",
    "\n",
    "df_events = (\n",
    "    df_events\n",
    "    .withColumn(\"Bowler\", trim(regexp_extract(\"event\", EVENT_PATTERN, 1)))\n",
    "    .withColumn(\"Batsman\", trim(regexp_extract(\"event\", EVENT_PATTERN, 2)))\n",
    "    .withColumn(\"runs_text\", trim(regexp_replace(\n",
    "        trim(regexp_extract(\"event\", EVENT_PATTERN, 3)), r\"\\(no ball\\)\", \"\")))\n",
    "    .withColumn(\"Extras\",\n",
    "        when(col(\"runs_text\").contains(\"wide\"), \"wide\")\n",
    "        .when(col(\"event\").contains(\"no ball\"), \"noball\")\n",
    "        .when(col(\"runs_text\").contains(\"leg bye\"), \"legbyes\")\n",
    "        .when(col(\"runs_text\").contains(\"bye\"), \"byes\")\n",
    "        .otherwise(\"normal\"))\n",
    "    .withColumn(\"over\", split(col(\"ball\").cast(\"string\"), \"\\\\.\")[0].cast(\"int\") + 1)\n",
    "    .withColumn(\"over_ball_number\", split(col(\"ball\").cast(\"string\"), \"\\\\.\")[1].cast(\"int\"))\n",
    "    .withColumn(\"runs\",\n",
    "        when(col(\"runs_text\") == \"OUT\", 0)\n",
    "        .when((col(\"runs_text\") == \"no run\") | (col(\"runs_text\") == \"\"), 0)\n",
    "        .when(col(\"runs_text\") == \"FOUR runs\", 4)\n",
    "        .when(col(\"runs_text\") == \"SIX runs\", 6)\n",
    "        .when(col(\"runs_text\").rlike(\"^\\\\d+\"),\n",
    "              regexp_extract(\"runs_text\", r\"^(\\d+)\", 1).cast(\"int\"))\n",
    "        .otherwise(0))\n",
    "    # Add no-ball penalty\n",
    "    .withColumn(\"runs\",\n",
    "        when(col(\"Extras\") == \"noball\", col(\"runs\") + 1).otherwise(col(\"runs\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d9cfdb4-3699-43be-91fd-50b6a1b660bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extract Dismissal Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3637ae68-7160-4ef8-bd2a-2fc4be5cff08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "is_out = col(\"runs_text\").contains(\"OUT\")\n",
    "comm = col(\"commentary\")\n",
    "\n",
    "PAT_RUNOUT  = r\"run out\\s*\\(\"\n",
    "PAT_CANDB   = r\"c & b\\s+\"\n",
    "PAT_CAUGHT  = r\"\\bc\\s+.+?\\s+b\\s+.+?\\s+\\d+\\s*\\(\"\n",
    "PAT_LBW     = r\"lbw\\s+b\\s+\"\n",
    "PAT_STUMPED = r\"(?:^|[^a-zA-Z])st\\s+[^a-zA-Z]*[A-Z].+?\\s+b\\s+.+?\\s+\\d+\\s*\\(\"\n",
    "PAT_BOWLED  = r\"(?:^|[^a-zA-Z])b\\s+\\w+\\s+\\d+\\s*\\(\"\n",
    "\n",
    "df_events = (\n",
    "    df_events\n",
    "    .withColumn(\"dismissal_method\",\n",
    "        when(~is_out, \"Not Out\")\n",
    "        .when(comm.rlike(PAT_RUNOUT), \"Run Out\")\n",
    "        .when(comm.rlike(PAT_CANDB), \"Caught & Bowled\")\n",
    "        .when(comm.rlike(PAT_CAUGHT), \"Caught\")\n",
    "        .when(comm.rlike(PAT_LBW), \"LBW\")\n",
    "        .when(comm.rlike(PAT_STUMPED), \"Stumped\")\n",
    "        .when(comm.rlike(PAT_BOWLED), \"Bowled\")\n",
    "        .otherwise(\"Not Out\"))\n",
    "    .withColumn(\"dismissed_by\",\n",
    "        when(~is_out, lit(None))\n",
    "        .when(comm.rlike(PAT_RUNOUT),\n",
    "             regexp_extract(\"commentary\", r\"run out\\s*\\(([^/\\)]+)\", 1))\n",
    "        .when(comm.rlike(PAT_CANDB),\n",
    "             regexp_extract(\"commentary\", r\"c & b\\s+(.+?)(?=\\s+\\d+\\s*\\()\", 1))\n",
    "        .when(comm.rlike(PAT_CAUGHT),\n",
    "             regexp_extract(\"commentary\", r\"\\bc\\s+(.+?)\\s+b\\s+\", 1))\n",
    "        .when(comm.rlike(PAT_LBW),\n",
    "             regexp_extract(\"commentary\", r\"lbw\\s+b\\s+(.+?)(?=\\s+\\d+\\s*\\()\", 1))\n",
    "        .when(comm.rlike(PAT_STUMPED),\n",
    "             regexp_extract(\"commentary\", r\"st\\s+(.+?)\\s+b\\s+\", 1))\n",
    "        .when(comm.rlike(PAT_BOWLED),\n",
    "             regexp_extract(\"commentary\", r\"(?:^|[^a-zA-Z])b\\s+(.+?)(?=\\s+\\d+\\s*\\()\", 1))\n",
    "        .otherwise(lit(None)))\n",
    "    .withColumn(\"dismissed_by\", trim(regexp_replace(\"dismissed_by\", \"â€ \", \"\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4aeac07-c0c0-48ae-a2f8-f4e039df7bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enrich with Team & Player Names + Backfill Super Overs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af8146b-20ab-4835-82cf-04a3ffea84bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Resolve names\n",
    "df_events = (\n",
    "    df_events.join(df_match_names, on=\"matchid\", how=\"left\")\n",
    "    .withColumn(\"team\", get_full_name(\"innings\", \"team_names\"))\n",
    "    .withColumn(\"batsman_full\", get_full_name(\"Batsman\", \"player_names\"))\n",
    "    .withColumn(\"bowler_full\", get_full_name(\"Bowler\", \"player_names\"))\n",
    ")\n",
    "\n",
    "# â”€â”€ Backfill Super Over rows â”€â”€\n",
    "non_super = ~col(\"team\").contains(\"Super Over\")\n",
    "\n",
    "# Batsman\n",
    "bat_map = df_events.filter(non_super & col(\"batsman_full\").isNotNull()) \\\n",
    "    .groupBy(\"Batsman\").agg(first(\"batsman_full\", ignorenulls=True).alias(\"_bat\"))\n",
    "df_events = df_events.join(bat_map, \"Batsman\", \"left\") \\\n",
    "    .withColumn(\"batsman_full\", coalesce(\"batsman_full\", \"_bat\")).drop(\"_bat\")\n",
    "\n",
    "# Bowler\n",
    "bow_map = df_events.filter(non_super & col(\"bowler_full\").isNotNull()) \\\n",
    "    .groupBy(\"Bowler\").agg(first(\"bowler_full\", ignorenulls=True).alias(\"_bow\"))\n",
    "df_events = df_events.join(bow_map, \"Bowler\", \"left\") \\\n",
    "    .withColumn(\"bowler_full\", coalesce(\"bowler_full\", \"_bow\")).drop(\"_bow\")\n",
    "\n",
    "# Super over team column + team backfill\n",
    "df_events = df_events.withColumn(\"super_over_team\",\n",
    "    when(col(\"team\").contains(\"Super Over\"), col(\"team\")).otherwise(lit(\"normal\")))\n",
    "\n",
    "team_map = df_events.filter(non_super & col(\"team\").isNotNull()) \\\n",
    "    .groupBy(\"batsman_full\").agg(first(\"team\", ignorenulls=True).alias(\"_team\"))\n",
    "df_events = df_events.join(team_map, \"batsman_full\", \"left\") \\\n",
    "    .withColumn(\"team\",\n",
    "        when(col(\"team\").contains(\"Super Over\"), col(\"_team\")).otherwise(col(\"team\"))) \\\n",
    "    .drop(\"_team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a61b32e7-c15c-4060-a5d1-d78a26cea612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Innings Score & Wickets Lost (Running Totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b572324-2db8-4aa7-93d7-78f95e0adef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Shared window spec for innings-level running totals\n",
    "innings_window = (\n",
    "    Window\n",
    "    .partitionBy(\"matchid\", \"team\", \"super_over_team\")\n",
    "    .orderBy(\"over\", \"match_ball_number\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "df_events = (\n",
    "    df_events\n",
    "    .withColumn(\"innings_score\", spark_sum(\"runs\").over(innings_window))\n",
    "    .withColumn(\"wickets_lost\",\n",
    "        spark_sum(\n",
    "            when(col(\"dismissal_method\") != \"Not Out\", 1).otherwise(0)\n",
    "        ).over(innings_window))\n",
    ")\n",
    "\n",
    "# Cache events â€” reused for Silver output + merge + display\n",
    "#df_events.cache()\n",
    "df_events.createOrReplaceTempView(\"match_events_temp\")\n",
    "df_events = spark.table(\"match_events_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31d41d4e-9ef0-48a1-a43b-6a3e4bf3f12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare & Merge Events into Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bf065b-1c94-48af-9812-b2e8f4b2f45e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events_silver = df_events.select(\n",
    "    \"Batchid\", \"matchid\", \"event\", \"ball\", \"over\", \"over_ball_number\",\"match_ball_number\",\n",
    "    \"runs\", \"innings_score\", \"wickets_lost\", \"commentary\",\n",
    "    col(\"bowler_full\").alias(\"bowler\"), col(\"batsman_full\").alias(\"batsman\"),\n",
    "    \"team\", \"is_super_over\", \"super_over_team\", \"runs_text\", \"Extras\",\n",
    "    \"dismissal_method\", \"dismissed_by\", \"source_file\", \"load_timestamp\",\n",
    "    current_timestamp().alias(\"silver_load_timestamp\")\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Events for Silver: {df_events_silver.count()}\")\n",
    "\n",
    "# Merge\n",
    "events_existed = merge_to_silver(\n",
    "    df_events_silver, SILVER_EVENTS, [\"matchid\", \"match_ball_number\", \"event\"]\n",
    ")\n",
    "\n",
    "# Propagate deletes\n",
    "propagate_deletes(\n",
    "    df_events_cdf, SILVER_EVENTS, [\"matchid\", \"match_ball_number\", \"event\"], events_existed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6dd9b16-c887-4347-99ff-415f23713e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Applying Transformations on Players Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f04e75e-d4b6-4d8c-8ac4-683c3ecd8706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dedup, Prepare & Merge Players into Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73443e1-d571-447e-9a85-a9aa28a74add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_players = dedup_dataframe(df_players_full, order_by_col=\"load_timestamp\", keep=\"latest\")\n",
    "\n",
    "df_players_silver = df_players.select(\n",
    "    \"Batchid\", \"matchid\", \"innings\", \"team\", \"player_name\",\n",
    "    \"batted\", \"batting_position\", \"player_type\", \"retired\",\n",
    "    \"not_out\", \"bowled\", \"source_file\", \"load_timestamp\",\n",
    "    current_timestamp().alias(\"silver_load_timestamp\")\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Players for Silver: {df_players_silver.count()}\")\n",
    "\n",
    "# Merge\n",
    "players_existed = merge_to_silver(\n",
    "    df_players_silver, SILVER_PLAYERS, [\"matchid\", \"player_name\", \"team\"]\n",
    ")\n",
    "\n",
    "# Propagate deletes\n",
    "propagate_deletes(\n",
    "    df_players_cdf, SILVER_PLAYERS, [\"matchid\", \"player_name\", \"team\"], players_existed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48cf798d-26f1-414c-ae3a-ce0ff72d3033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Applying Transformations on Match Metadata Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f74e35a-d1c0-42fb-a125-62ee6cd3163f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dedup, Clean Text, Parse Dates/Times, Resolve Timezones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5328364-37e7-4c43-a8ef-6ecf4be7ed61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dedup\n",
    "df_metadata = dedup_dataframe(df_metadata_full, order_by_col=\"load_timestamp\", keep=\"latest\")\n",
    "\n",
    "# Split toss: \"Team, elected to bat first\" â†’ toss + decision\n",
    "toss_parts = split(col(\"toss\"), \",\")\n",
    "df_metadata = (\n",
    "    df_metadata\n",
    "    .withColumn(\"_parts\", toss_parts)\n",
    "    .withColumn(\"toss\", trim(col(\"_parts\")[0]))\n",
    "    .withColumn(\"decision\",\n",
    "        when(size(col(\"_parts\")) >= 2, trim(col(\"_parts\")[1])).otherwise(lit(None)))\n",
    "    .drop(\"_parts\")\n",
    ")\n",
    "\n",
    "# Remove repetitive text in columns\n",
    "df_metadata = dedup_column_values(df_metadata, [\n",
    "    \"series\", \"player_of_the_match\", \"player_of_the_series\",\n",
    "    \"t20_debut\", \"t20i_debut\", \"umpires\", \"tv_umpire\",\n",
    "    \"reserve_umpire\", \"match_referee\"\n",
    "])\n",
    "\n",
    "# Parse match_days â†’ match_date\n",
    "df_metadata = df_metadata.withColumn(\"match_date\",\n",
    "    to_date(trim(regexp_extract(\"match_days\", r\"^(\\d+\\s+\\w+\\s+\\d{4})\", 1)), \"d MMMM yyyy\"))\n",
    "\n",
    "# Parse hours_of_play â†’ structured time columns\n",
    "time_extractions = {\n",
    "    \"start_time\":           r\"^(\\d+\\.\\d+)\\s+start\",\n",
    "    \"first_innings_start\":  r\"First Session\\s+(\\d+\\.\\d+)\",\n",
    "    \"first_innings_end\":    r\"First Session\\s+\\d+\\.\\d+-(\\d+\\.\\d+)\",\n",
    "    \"interval_start\":       r\"Interval\\s+(\\d+\\.\\d+)\",\n",
    "    \"interval_end\":         r\"Interval\\s+\\d+\\.\\d+-(\\d+\\.\\d+)\",\n",
    "    \"second_innings_start\": r\"Second Session\\s+(\\d+\\.\\d+)\",\n",
    "    \"second_innings_end\":   r\"Second Session\\s+\\d+\\.\\d+-(\\d+\\.\\d+)\",\n",
    "}\n",
    "\n",
    "for col_name, pattern in time_extractions.items():\n",
    "    df_metadata = df_metadata.withColumn(col_name,\n",
    "        when(\n",
    "            regexp_extract(\"hours_of_play_local_time\", pattern, 1) == \"\", lit(None)\n",
    "        ).otherwise(\n",
    "            regexp_extract(\"hours_of_play_local_time\", pattern, 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Timezone: geocode grounds â†’ timezone\n",
    "timezone_map = build_ground_timezone_map(df_metadata, ground_col=\"ground\")\n",
    "df_metadata = apply_timezone_map(df_metadata, timezone_map)\n",
    "\n",
    "# Convert local times to UTC\n",
    "for local_col, utc_col in [\n",
    "    (\"start_time\",            \"match_start_utc\"),\n",
    "    (\"first_innings_start\",   \"first_innings_start_utc\"),\n",
    "    (\"first_innings_end\",     \"first_innings_end_utc\"),\n",
    "    (\"second_innings_start\",  \"second_innings_start_utc\"),\n",
    "    (\"second_innings_end\",    \"second_innings_end_utc\"),\n",
    "]:\n",
    "    df_metadata = local_time_to_utc(df_metadata, local_col, \"match_date\", \"local_timezone\", utc_col)\n",
    "\n",
    "# Resolve team names for first/second innings\n",
    "df_metadata = (\n",
    "    df_metadata.join(df_match_names, on=\"matchid\", how=\"left\")\n",
    "    .withColumn(\"first_innings\", get_full_name(\"first_innings\", \"team_names\"))\n",
    "    .withColumn(\"second_innings\", get_full_name(\"second_innings\", \"team_names\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "096c19e4-13df-4628-8558-141ae6188cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare & Merge Metadata into Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a06d26-9995-47d1-9ec2-e6929455706c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_metadata_silver = df_metadata.select(\n",
    "    \"Batchid\", \"matchid\", \"ground\", \"toss\", \"decision\", \"series\",\n",
    "    \"season\", \"player_of_the_match\", \"player_of_the_series\",\n",
    "    \"t20_debut\", \"t20i_debut\", \"umpires\", \"tv_umpire\",\n",
    "    \"reserve_umpire\", \"match_referee\", \"points\", \"player_replacements\",\n",
    "    \"match_number\", \"match_date\", \"match_start_utc\",\n",
    "    \"first_innings_start_utc\", \"first_innings_end_utc\",\n",
    "    \"second_innings_start_utc\", \"second_innings_end_utc\",\n",
    "    \"first_innings\", \"second_innings\",\n",
    "    \"has_super_over\", \"super_over_count\", \"series_result\",\n",
    "    \"source_file\", \"load_timestamp\",\n",
    "    current_timestamp().alias(\"silver_load_timestamp\")\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Metadata for Silver: {df_metadata_silver.count()}\")\n",
    "\n",
    "# Merge\n",
    "metadata_existed = merge_to_silver(df_metadata_silver, SILVER_METADATA, [\"matchid\"])\n",
    "\n",
    "# Propagate deletes\n",
    "propagate_deletes(df_metadata_cdf, SILVER_METADATA, [\"matchid\"], metadata_existed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ae7f1fe-62d6-4c72-807f-6d7114a5042b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Update CDF Checkpoints\n",
    "\n",
    "Checkpoints updated only after all Silver tables are successfully written.\n",
    "If the notebook fails mid-run, next execution re-reads from the last\n",
    "successful version (at-least-once + merge = idempotent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6e2952-1e6f-436b-b109-61f73b4866cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name, version in [\n",
    "    (BRONZE_EVENTS, events_version),\n",
    "    (BRONZE_PLAYERS, players_version),\n",
    "    (BRONZE_METADATA, metadata_version)\n",
    "]:\n",
    "    if version is not None:\n",
    "        update_checkpoint(table_name, version)\n",
    "        print(f\"  âœ… {table_name} â†’ version {version}\")\n",
    "\n",
    "# Release cached DataFrames\n",
    "#df_match_names.unpersist()\n",
    "#df_events.unpersist()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Incremental Silver load via CDF complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b616c4-6376-462a-ac32-f42c6d9dd9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run DQ checks after ingestion completes\n",
    "dq_result = dbutils.notebook.run(\n",
    "    \"/Workspace/Shared/cricketCommentaryScraper/DataQualityRulesSilverLayer\",\n",
    "    timeout_seconds=600,  # 10 min timeout,\n",
    "    arguments       = {\n",
    "        \"catalog_name\":  CATALOG\n",
    "    }\n",
    ")\n",
    "print(f\"DQ Result: {dq_result}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CricinfoCommentaryParser_SilverLayer",
   "widgets": {
    "catalog_name": {
     "currentValue": "T20_catalog",
     "nuid": "4e5f5fea-af9d-4f0c-869d-42b56fdaabec",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "T20_catalog",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "T20_catalog",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
