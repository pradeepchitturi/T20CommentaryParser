{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60481551-5057-4ad2-9695-905314e4fed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Quality Rules - Cricket Commentary Data Pipeline\n",
    "####This notebook defines and executes **comprehensive data quality checks** on the bronze layer tables\n",
    "####ingested via Auto Loader. It runs after every pipeline execution and logs results to a DQ audit table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb4de775-5503-4c3e-bef0-c5a78b636766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**Tables Validated:**\n",
    "####`T20_catalog.bronze.match_events` (ball-by-ball CSV)\n",
    "####`T20_catalog.bronze.match_metadata` (match-level JSON)\n",
    "####`T20_catalog.bronze.match_players` (player-level CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f137b75f-3b51-4a2f-9f42-cd5bba504f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "916e6be5-5cbb-4493-82f7-568e6de6e5a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Job Parameters ────────────────────────────────────────────────────────────\n",
    "# Default values are used during interactive runs.\n",
    "# Databricks Job overrides these at runtime via the Parameters section.\n",
    "# Key names here must match exactly what the Job defines.\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"catalog_name\", \"T20_catalog_dev\",\n",
    "    \"Catalog Name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cbc10e7-63eb-4568-83cf-ac2bfa91611f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, isnull, isnan, lit, sum as _sum, avg, min as _min, max as _max,\n",
    "    length, trim, regexp_extract, current_timestamp, to_timestamp, datediff,\n",
    "    countDistinct, expr, upper, lower, array_contains, size, coalesce\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "CATALOG_NAME  = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "FULL_SCHEMA = f\"{CATALOG_NAME}.{SCHEMA_NAME}\"\n",
    "\n",
    "# DQ Audit table\n",
    "DQ_AUDIT_TABLE = f\"{FULL_SCHEMA}.dq_audit_log\"\n",
    "\n",
    "# Tables to validate\n",
    "MATCH_EVENTS_TABLE = f\"{FULL_SCHEMA}.match_events\"\n",
    "MATCH_METADATA_TABLE = f\"{FULL_SCHEMA}.match_metadata\"\n",
    "MATCH_PLAYERS_TABLE = f\"{FULL_SCHEMA}.match_players\"\n",
    "\n",
    "# Timestamp for this run\n",
    "run_timestamp = datetime.now(timezone.utc)\n",
    "run_id = run_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"DQ Run ID: {run_id}\")\n",
    "print(f\"Run Timestamp: {run_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdb2a410-2f67-4d3a-aaf7-702f3370d563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. DQ Audit Table Setup & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62dde2a8-0ed2-4dc2-9f42-47cd082e41cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create audit table if not exists\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DQ_AUDIT_TABLE} (\n",
    "        run_id              STRING      COMMENT 'Unique identifier for this DQ run',\n",
    "        run_timestamp       TIMESTAMP   COMMENT 'When the DQ check was executed',\n",
    "        table_name          STRING      COMMENT 'Fully qualified table name being checked',\n",
    "        rule_category       STRING      COMMENT 'Category: completeness, validity, uniqueness, consistency, timeliness, accuracy, integrity',\n",
    "        rule_name           STRING      COMMENT 'Descriptive name of the DQ rule',\n",
    "        rule_description    STRING      COMMENT 'Detailed description of what the rule checks',\n",
    "        total_records       LONG        COMMENT 'Total records in scope',\n",
    "        passed_records      LONG        COMMENT 'Records that passed the check',\n",
    "        failed_records      LONG        COMMENT 'Records that failed the check',\n",
    "        pass_percentage     DOUBLE      COMMENT 'Percentage of records that passed',\n",
    "        status              STRING      COMMENT 'PASS, WARN, FAIL based on thresholds',\n",
    "        threshold_pct       DOUBLE      COMMENT 'Minimum acceptable pass percentage',\n",
    "        details             STRING      COMMENT 'Additional details or sample failures'\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Data quality audit log for IPL cricket pipeline'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ DQ audit table ready: {DQ_AUDIT_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6917530-eb38-47f2-8cc5-b08a1793a64b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_dq_result(table_name, rule_category, rule_name, rule_description,\n",
    "                  total_records, passed_records, threshold_pct=100.0, details=\"\"):\n",
    "    \"\"\"Log a single DQ check result to the audit table.\"\"\"\n",
    "    \n",
    "    failed_records = total_records - passed_records\n",
    "    pass_pct = (passed_records / total_records * 100) if total_records > 0 else 0.0\n",
    "    \n",
    "    if pass_pct >= threshold_pct:\n",
    "        status = \"PASS\"\n",
    "    elif pass_pct >= (threshold_pct - 5):\n",
    "        status = \"WARN\"\n",
    "    else:\n",
    "        status = \"FAIL\"\n",
    "    \n",
    "    icon = \"✓\" if status == \"PASS\" else (\"⚠\" if status == \"WARN\" else \"✗\")\n",
    "    print(f\"  {icon} [{status}] {rule_name}: {pass_pct:.2f}% passed ({failed_records} failures)\")\n",
    "    \n",
    "    row = spark.createDataFrame([{\n",
    "        \"run_id\": run_id,\n",
    "        \"run_timestamp\": run_timestamp,\n",
    "        \"table_name\": table_name,\n",
    "        \"rule_category\": rule_category,\n",
    "        \"rule_name\": rule_name,\n",
    "        \"rule_description\": rule_description,\n",
    "        \"total_records\": int(total_records),\n",
    "        \"passed_records\": int(passed_records),\n",
    "        \"failed_records\": int(failed_records),\n",
    "        \"pass_percentage\": round(pass_pct, 2),\n",
    "        \"status\": status,\n",
    "        \"threshold_pct\": threshold_pct,\n",
    "        \"details\": details[:500]  # truncate long details\n",
    "    }])\n",
    "    \n",
    "    row.write.mode(\"append\").saveAsTable(DQ_AUDIT_TABLE)\n",
    "    return status\n",
    "\n",
    "\n",
    "def get_null_count(df, column):\n",
    "    \"\"\"Count nulls, empty strings, and 'null' string values.\"\"\"\n",
    "    return df.filter(\n",
    "        col(column).isNull() | \n",
    "        (trim(col(column)) == \"\") | \n",
    "        (lower(trim(col(column))) == \"null\") |\n",
    "        (lower(trim(col(column))) == \"none\") |\n",
    "        (lower(trim(col(column))) == \"n/a\")\n",
    "    ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bebc5c50-040d-456f-946d-a1ed69e7c7cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Load Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a0f424-bb24-495a-bd55-7c2281fe747d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events = spark.table(MATCH_EVENTS_TABLE)\n",
    "df_metadata = spark.table(MATCH_METADATA_TABLE)\n",
    "df_players = spark.table(MATCH_PLAYERS_TABLE)\n",
    "\n",
    "events_count = df_events.count()\n",
    "metadata_count = df_metadata.count()\n",
    "players_count = df_players.count()\n",
    "\n",
    "print(f\"match_events:   {events_count:,} rows\")\n",
    "print(f\"match_metadata: {metadata_count:,} rows\")\n",
    "print(f\"match_players:  {players_count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f08c16-2ede-4e11-92ed-bd663c58399c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. DATA QUALITY RULES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84c22410-82f6-4719-afdb-27f75a9d6eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.1 COMPLETENESS RULES\n",
    " _Checks that required fields are populated (not null, not empty)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113d1512-f340-46f8-b0a5-1dbaf86a5bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: COMPLETENESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_EVENTS - Completeness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_EVENTS_TABLE} ---\")\n",
    "\n",
    "# Rule C-E01: matchid must not be null\n",
    "null_matchid = df_events.filter(col(\"matchid\").isNull()).count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E01: matchid_not_null\",\n",
    "    \"matchid must be populated for every event row\",\n",
    "    events_count, events_count - null_matchid, 100.0)\n",
    "\n",
    "# Rule C-E02: ball must not be null\n",
    "null_ball = df_events.filter(col(\"ball\").isNull() | (trim(col(\"ball\")) == \"\")).count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E02: ball_not_null\",\n",
    "    \"ball identifier (e.g., 0.1, 1.2) must be populated\",\n",
    "    events_count, events_count - null_ball, 100.0)\n",
    "\n",
    "# Rule C-E03: innings must not be null\n",
    "null_innings = df_events.filter(col(\"innings\").isNull() | (trim(col(\"innings\")) == \"\")).count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E03: innings_not_null\",\n",
    "    \"innings must be populated (innings_1 or innings_2)\",\n",
    "    events_count, events_count - null_innings, 100.0)\n",
    "\n",
    "# Rule C-E04: commentary must not be null\n",
    "null_commentary = get_null_count(df_events, \"commentary\")\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E04: commentary_not_null\",\n",
    "    \"Ball-by-ball commentary text should be present\",\n",
    "    events_count, events_count - null_commentary, 75.0)\n",
    "\n",
    "# Rule C-E05: bowler must not be null\n",
    "null_bowler = get_null_count(df_events, \"bowler\")\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E05: bowler_not_null\",\n",
    "    \"Bowler name should be present for every delivery\",\n",
    "    events_count, events_count - null_bowler, 95.0)\n",
    "\n",
    "# Rule C-E06: batsman must not be null\n",
    "null_batsman = get_null_count(df_events, \"batsman\")\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E06: batsman_not_null\",\n",
    "    \"Batsman name should be present for every delivery\",\n",
    "    events_count, events_count - null_batsman, 95.0)\n",
    "\n",
    "# Rule C-E07: event must not be null\n",
    "null_event = get_null_count(df_events, \"event\")\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E08: event_not_null\",\n",
    "    \"Event type (run, wicket, wide, etc.) should be populated\",\n",
    "    events_count, events_count - null_event, 90.0)\n",
    "\n",
    "# Rule C-E08: source_file must not be null (Auto Loader metadata)\n",
    "null_source = df_events.filter(col(\"source_file\").isNull()).count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E09: source_file_not_null\",\n",
    "    \"Auto Loader source file path must be captured\",\n",
    "    events_count, events_count - null_source, 100.0)\n",
    "\n",
    "# Rule C-E09: load_timestamp must not be null\n",
    "null_load_ts = df_events.filter(col(\"load_timestamp\").isNull()).count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"completeness\", \"C-E10: load_timestamp_not_null\",\n",
    "    \"Load timestamp from Auto Loader must be present\",\n",
    "    events_count, events_count - null_load_ts, 100.0)\n",
    "\n",
    "    # ──────────────────────────────────────────────\n",
    "# MATCH_METADATA - Completeness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_METADATA_TABLE} ---\")\n",
    "\n",
    "# Rule C-M01: matchid not null\n",
    "null_m_matchid = df_metadata.filter(col(\"matchid\").isNull()).count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M01: matchid_not_null\",\n",
    "    \"matchid must be populated for every metadata record\",\n",
    "    metadata_count, metadata_count - null_m_matchid, 100.0)\n",
    "\n",
    "# Rule C-M02: ground not null\n",
    "null_ground = get_null_count(df_metadata, \"ground\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M02: ground_not_null\",\n",
    "    \"Match venue/stadium must be populated\",\n",
    "    metadata_count, metadata_count - null_ground, 100.0)\n",
    "\n",
    "# Rule C-M03: series not null\n",
    "null_series = get_null_count(df_metadata, \"series\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M03: series_not_null\",\n",
    "    \"Series/tournament name must be populated\",\n",
    "    metadata_count, metadata_count - null_series, 100.0)\n",
    "\n",
    "# Rule C-M04: season not null\n",
    "null_season = get_null_count(df_metadata, \"season\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M04: season_not_null\",\n",
    "    \"Season/year must be populated\",\n",
    "    metadata_count, metadata_count - null_season, 100.0)\n",
    "\n",
    "# Rule C-M05: match_referee not null\n",
    "null_match_referee = get_null_count(df_metadata, \"match_referee\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M05: match_referee_not_null\",\n",
    "    \"Match winner must be populated (unless no result)\",\n",
    "    metadata_count, metadata_count - null_match_referee, 95.0)\n",
    "\n",
    "# Rule C-M06: toss not null\n",
    "null_toss = get_null_count(df_metadata, \"toss\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M06: toss_not_null\",\n",
    "    \"Toss result must be populated\",\n",
    "    metadata_count, metadata_count - null_toss, 100.0)\n",
    "\n",
    "# Rule C-M07: first_innings_score not null\n",
    "null_fis = get_null_count(df_metadata, \"first_innings\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M07: first_innings_not_null\",\n",
    "    \"First innings score must be populated for completed matches\",\n",
    "    metadata_count, metadata_count - null_fis, 95.0)\n",
    "\n",
    "# Rule C-M08: second_innings_score not null\n",
    "null_sis = get_null_count(df_metadata, \"second_innings\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M08: second_innings_not_null\",\n",
    "    \"Second innings score must be populated for completed matches\",\n",
    "    metadata_count, metadata_count - null_sis, 95.0)\n",
    "\n",
    "# Rule C-M09: source_file not null\n",
    "null_m_source = df_metadata.filter(col(\"source_file\").isNull()).count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M09: source_file_not_null\",\n",
    "    \"Source file path from Auto Loader must be captured\",\n",
    "    metadata_count, metadata_count - null_m_source, 100.0)\n",
    "\n",
    "# Rule C-M10: umpire not null\n",
    "null_umpires = get_null_count(df_metadata, \"umpires\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M10: umpires_not_null\",\n",
    "    \"Match winner must be populated (unless no result)\",\n",
    "    metadata_count, metadata_count - null_umpires, 95.0)\n",
    "\n",
    "# Rule C-M11: tv_umpire not null\n",
    "null_tv_umpire = get_null_count(df_metadata, \"tv_umpire\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M11: tv_umpire_not_null\",\n",
    "    \"Match winner must be populated (unless no result)\",\n",
    "    metadata_count, metadata_count - null_tv_umpire, 95.0)\n",
    "\n",
    "# Rule C-M12: match_number not null\n",
    "null_match_number = get_null_count(df_metadata, \"match_number\")\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"completeness\", \"C-M12: match_number_not_null\",\n",
    "    \"Match winner must be populated (unless no result)\",\n",
    "    metadata_count, metadata_count - null_match_number, 95.0)\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_PLAYERS - Completeness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_PLAYERS_TABLE} ---\")\n",
    "\n",
    "# Rule C-P01: matchid not null\n",
    "null_p_matchid = df_players.filter(col(\"matchid\").isNull()).count()\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"completeness\", \"C-P01: matchid_not_null\",\n",
    "    \"matchid must be populated for every player record\",\n",
    "    players_count, players_count - null_p_matchid, 100.0)\n",
    "\n",
    "# Rule C-P02: player_name not null\n",
    "null_p_player_name = df_players.filter(col(\"matchid\").isNull()).count()\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"completeness\", \"C-P02: player_name_not_null\",\n",
    "    \"matchid must be populated for every player record\",\n",
    "    players_count, players_count - null_p_player_name, 100.0)\n",
    "\n",
    "# Rule C-P03: team not null\n",
    "null_p_team = df_players.filter(col(\"matchid\").isNull()).count()\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"completeness\", \"C-P03: team_not_null\",\n",
    "    \"matchid must be populated for every player record\",\n",
    "    players_count, players_count - null_p_team, 100.0)\n",
    "\n",
    "# Rule C-P04: innings not null\n",
    "if \"innings\" in df_players.columns:\n",
    "    null_p_innings = get_null_count(df_players, \"innings\")\n",
    "    log_dq_result(MATCH_PLAYERS_TABLE, \"completeness\", \"C-P04: innings_not_null\",\n",
    "        \"Innings assignment must be populated for every player\",\n",
    "        players_count, players_count - null_p_innings, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7795108b-6edb-4265-837c-59bfbb1571b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2 VALIDITY / FORMAT RULES\n",
    "_Checks that values conform to expected formats, ranges, and domains._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ffa667-c0a6-49ca-8b94-33a35bc495b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: VALIDITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_EVENTS - Validity\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_EVENTS_TABLE} ---\")\n",
    "\n",
    "# Rule V-E01: matchid must be a positive integer\n",
    "invalid_matchid = df_events.filter(\n",
    "    (col(\"matchid\").isNull()) | (col(\"matchid\") <= 0)\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E01: matchid_positive_integer\",\n",
    "    \"matchid must be a positive integer value\",\n",
    "    events_count, events_count - invalid_matchid, 100.0)\n",
    "\n",
    "# Rule V-E02: ball must follow cricket over.ball format (e.g., 0.1, 19.6)\n",
    "invalid_ball = df_events.filter(\n",
    "    ~col(\"ball\").rlike(r\"^\\d{1,2}\\.\\d{1}$\")\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E02: ball_format_valid\",\n",
    "    \"ball must follow over.ball_number format (e.g., 0.1, 15.3, 19.6)\",\n",
    "    events_count, events_count - invalid_ball, 98.0,\n",
    "    \"Expected format: D.D or DD.D where over 0-19, ball 1-6+\")\n",
    "\n",
    "# Rule V-E03: over number (from ball) must be 0-19 for T20\n",
    "df_events_with_over = df_events.withColumn(\n",
    "    \"over_num\", regexp_extract(col(\"ball\"), r\"^(\\d{1,2})\\.\", 1).cast(\"int\")\n",
    ")\n",
    "invalid_over = df_events_with_over.filter(\n",
    "    (col(\"over_num\").isNull()) | (col(\"over_num\") < 0) | (col(\"over_num\") > 300)\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E03: over_number_range_0_19\",\n",
    "    \"Over number extracted from ball must be between 0 and 300 (ODI,T20 and test format)\",\n",
    "    events_count, events_count - invalid_over, 98.0)\n",
    "\n",
    "# Rule V-E04: ball number within over must be 1-9 (6 legal + extras)\n",
    "df_events_with_ball_num = df_events.withColumn(\n",
    "    \"ball_in_over\", regexp_extract(col(\"ball\"), r\"\\.(\\d{1})$\", 1).cast(\"int\")\n",
    ")\n",
    "invalid_ball_num = df_events_with_ball_num.filter(\n",
    "    (col(\"ball_in_over\").isNull()) | (col(\"ball_in_over\") < 1) | (col(\"ball_in_over\") > 6)\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E04: ball_in_over_range_1_6\",\n",
    "    \"Ball number within over should be 1-6 (extras will have the same ball number)\",\n",
    "    events_count, events_count - invalid_ball_num, 98.0)\n",
    "\n",
    "# Rule V-E05: bowler name should be alphabetic (allow spaces, dots, apostrophes)\n",
    "invalid_bowler_name = df_events.filter(\n",
    "    col(\"bowler\").isNotNull() & ~col(\"bowler\").rlike(r\"^[A-Za-z\\s\\.\\'\\-]+$\")\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E06: bowler_name_format\",\n",
    "    \"Bowler name should contain only letters, spaces, dots, hyphens, apostrophes\",\n",
    "    events_count, events_count - invalid_bowler_name, 80.0)\n",
    "\n",
    "# Rule V-E06: batsman name should be alphabetic\n",
    "invalid_batsman_name = df_events.filter(\n",
    "    col(\"batsman\").isNotNull() & ~col(\"batsman\").rlike(r\"^[A-Za-z\\s\\.\\'\\-]+$\")\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E07: batsman_name_format\",\n",
    "    \"Batsman name should contain only letters, spaces, dots, hyphens, apostrophes\",\n",
    "    events_count, events_count - invalid_batsman_name, 80.0)\n",
    "\n",
    "# Rule V-E07: source_file should contain matchid= pattern (ADLS path validation)\n",
    "invalid_source_path = df_events.filter(\n",
    "    ~col(\"source_file\").contains(\"matchid=\")\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E09: source_file_contains_matchid\",\n",
    "    \"Source file path must contain 'matchid=' partition pattern from ADLS\",\n",
    "    events_count, events_count - invalid_source_path, 100.0)\n",
    "\n",
    "# Rule V-E08: matchid extracted from path should match data matchid\n",
    "df_events_path_check = df_events.withColumn(\n",
    "    \"path_matchid\",\n",
    "    regexp_extract(col(\"source_file\"), r\"matchid=(\\d+)\", 1).cast(\"int\")\n",
    ")\n",
    "mismatched_matchid = df_events_path_check.filter(\n",
    "    col(\"matchid\") != col(\"path_matchid\")\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"validity\", \"V-E10: matchid_matches_source_path\",\n",
    "    \"matchid in data must match matchid extracted from ADLS file path\",\n",
    "    events_count, events_count - mismatched_matchid, 100.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_METADATA - Validity\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_METADATA_TABLE} ---\")\n",
    "\n",
    "# Rule V-M01: matchid must be positive integer\n",
    "invalid_m_matchid = df_metadata.filter(\n",
    "    (col(\"matchid\").isNull()) | (col(\"matchid\") <= 0)\n",
    ").count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"validity\", \"V-M01: matchid_positive_integer\",\n",
    "    \"matchid must be a positive integer\",\n",
    "    metadata_count, metadata_count - invalid_m_matchid, 100.0)\n",
    "\n",
    "# Rule V-M02: batting_first != batting_second (different teams)\n",
    "same_teams = df_metadata.filter(\n",
    "    col(\"first_innings\").isNotNull() & \n",
    "    col(\"second_innings\").isNotNull() &\n",
    "    (trim(col(\"first_innings\")) == trim(col(\"second_innings\")))\n",
    ").count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"validity\", \"V-M07: teams_are_different\",\n",
    "    \"batting_first and batting_second must be different teams\",\n",
    "    metadata_count, metadata_count - same_teams, 100.0)\n",
    "\n",
    "# Rule V-M03: season should be a valid year (2008-2030 for IPL)\n",
    "invalid_season = df_metadata.filter(\n",
    "    col(\"season\").isNotNull() & (\n",
    "        ~col(\"season\").rlike(r\"^\\d{4}(/\\d{2})?$\") |\n",
    "        (regexp_extract(col(\"season\"), r\"^(\\d{4})\", 1).cast(\"int\") < 2008) |\n",
    "        (regexp_extract(col(\"season\"), r\"^(\\d{4})\", 1).cast(\"int\") > 2030)\n",
    "    )\n",
    ").count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"validity\", \"V-M02: season_valid_year\",\n",
    "    \"Season must be a valid year between 2008-2030 for IPL\",\n",
    "    metadata_count, metadata_count - invalid_season, 100.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_PLAYERS - Validity\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_PLAYERS_TABLE} ---\")\n",
    "\n",
    "# Rule V-P01: matchid must be positive integer\n",
    "invalid_p_matchid = df_players.filter(\n",
    "    (col(\"matchid\").isNull()) | (col(\"matchid\") <= 0)\n",
    ").count()\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"validity\", \"V-P01: matchid_positive_integer\",\n",
    "    \"matchid must be a positive integer\",\n",
    "    players_count, players_count - invalid_p_matchid, 100.0)\n",
    "\n",
    "# Rule V-P02: innings value must be valid\n",
    "if \"innings\" in df_players.columns:\n",
    "    invalid_p_innings = df_players.filter(\n",
    "        col(\"innings\").isNotNull() &\n",
    "        ~lower(trim(col(\"innings\"))).isin([\"innings_1\", \"innings_2\"])\n",
    "    ).count()\n",
    "    log_dq_result(MATCH_PLAYERS_TABLE, \"validity\", \"V-P02: innings_value_valid\",\n",
    "        \"innings must be 'innings_1' or 'innings_2'\",\n",
    "        players_count, players_count - invalid_p_innings, 100.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c063d90-987d-4666-bcd5-0d1637348c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3 UNIQUENESS RULES\n",
    " _Checks for duplicate records and natural key integrity._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce37d59f-8479-4a5f-8e1f-a30ff82083e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: UNIQUENESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_EVENTS - Uniqueness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_EVENTS_TABLE} ---\")\n",
    "\n",
    "\n",
    "# Rule U-E01: No fully duplicate rows\n",
    "full_distinct = df_events.distinct().count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"uniqueness\", \"U-E02: no_full_duplicates\",\n",
    "    \"There should be no fully identical rows across all columns\",\n",
    "    events_count, full_distinct, 99.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_METADATA - Uniqueness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_METADATA_TABLE} ---\")\n",
    "\n",
    "# Rule U-M01: matchid should be unique (one metadata record per match)\n",
    "distinct_m_matchid = df_metadata.select(\"matchid\").distinct().count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"uniqueness\", \"U-M01: matchid_unique\",\n",
    "    \"Each match should have exactly one metadata record\",\n",
    "    metadata_count, distinct_m_matchid, 100.0,\n",
    "    f\"Total: {metadata_count}, Distinct matchids: {distinct_m_matchid}\")\n",
    "\n",
    "# Rule U-M02: No fully duplicate metadata rows\n",
    "full_distinct_m = df_metadata.distinct().count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"uniqueness\", \"U-M02: no_full_duplicates\",\n",
    "    \"There should be no fully identical metadata rows\",\n",
    "    metadata_count, full_distinct_m, 100.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# MATCH_PLAYERS - Uniqueness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {MATCH_PLAYERS_TABLE} ---\")\n",
    "\n",
    "# Rule U-P01: Each player appears once per (matchid, innings)\n",
    "player_col = None\n",
    "for c in [\"player\", \"player_name\", \"name\"]:\n",
    "    if c in df_players.columns:\n",
    "        player_col = c\n",
    "        break\n",
    "\n",
    "if player_col and \"innings\" in df_players.columns:\n",
    "    total_players = players_count\n",
    "    distinct_players = df_players.select(\"matchid\", \"innings\", player_col).distinct().count()\n",
    "    log_dq_result(MATCH_PLAYERS_TABLE, \"uniqueness\", \"U-P01: player_per_match_innings_unique\",\n",
    "        f\"Each ({player_col}, matchid, innings) combination should be unique\",\n",
    "        total_players, distinct_players, 99.0)\n",
    "\n",
    "# Rule U-P02: No fully duplicate player rows\n",
    "full_distinct_p = df_players.distinct().count()\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"uniqueness\", \"U-P02: no_full_duplicates\",\n",
    "    \"There should be no fully identical player rows\",\n",
    "    players_count, full_distinct_p, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd86b389-18d7-429f-bbed-ed153d4dd5f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.4 CONSISTENCY / CROSS-TABLE INTEGRITY RULES\n",
    "_Checks referential integrity and logical consistency across tables._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f168d50-9976-4d28-b7c0-0668c2d45887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: CONSISTENCY / INTEGRITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rule I-01: Every matchid in match_events must exist in match_metadata\n",
    "events_matchids = df_events.select(\"matchid\").distinct()\n",
    "metadata_matchids = df_metadata.select(\"matchid\").distinct()\n",
    "orphan_event_matchids = events_matchids.join(metadata_matchids, \"matchid\", \"left_anti\")\n",
    "orphan_count = orphan_event_matchids.count()\n",
    "total_event_matchids = events_matchids.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"integrity\", \"I-01: events_matchid_in_metadata\",\n",
    "    \"Every matchid in match_events must have a corresponding record in match_metadata\",\n",
    "    total_event_matchids, total_event_matchids - orphan_count, 100.0,\n",
    "    f\"Orphan matchids in events (no metadata): {orphan_count}\")\n",
    "\n",
    "# Rule I-02: Every matchid in match_players must exist in match_metadata\n",
    "players_matchids = df_players.select(\"matchid\").distinct()\n",
    "orphan_player_matchids = players_matchids.join(metadata_matchids, \"matchid\", \"left_anti\")\n",
    "orphan_p_count = orphan_player_matchids.count()\n",
    "total_player_matchids = players_matchids.count()\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"integrity\", \"I-02: players_matchid_in_metadata\",\n",
    "    \"Every matchid in match_players must have a corresponding record in match_metadata\",\n",
    "    total_player_matchids, total_player_matchids - orphan_p_count, 100.0,\n",
    "    f\"Orphan matchids in players (no metadata): {orphan_p_count}\")\n",
    "\n",
    "# Rule I-03: Every matchid in match_metadata should have events\n",
    "metadata_no_events = metadata_matchids.join(events_matchids, \"matchid\", \"left_anti\")\n",
    "missing_events_count = metadata_no_events.count()\n",
    "total_meta_matchids = metadata_matchids.count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"integrity\", \"I-03: metadata_has_events\",\n",
    "    \"Every match in metadata should have corresponding ball-by-ball events\",\n",
    "    total_meta_matchids, total_meta_matchids - missing_events_count, 95.0,\n",
    "    f\"Matches with metadata but no events: {missing_events_count}\")\n",
    "\n",
    "# Rule I-04: Every matchid in match_metadata should have players\n",
    "metadata_no_players = metadata_matchids.join(players_matchids, \"matchid\", \"left_anti\")\n",
    "missing_players_count = metadata_no_players.count()\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"integrity\", \"I-04: metadata_has_players\",\n",
    "    \"Every match in metadata should have corresponding player records\",\n",
    "    total_meta_matchids, total_meta_matchids - missing_players_count, 95.0,\n",
    "    f\"Matches with metadata but no players: {missing_players_count}\")\n",
    "\n",
    "# Rule I-05: matchid sets should be identical across all 3 tables\n",
    "all_three_match = events_matchids.join(metadata_matchids, \"matchid\", \"inner\") \\\n",
    "                                  .join(players_matchids, \"matchid\", \"inner\")\n",
    "all_matchids = events_matchids.union(metadata_matchids).union(players_matchids).distinct()\n",
    "complete_count = all_three_match.count()\n",
    "total_unique_matchids = all_matchids.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"integrity\", \"I-05: all_tables_matchid_aligned\",\n",
    "    \"All three tables should have the same set of matchids (complete data)\",\n",
    "    total_unique_matchids, complete_count, 95.0,\n",
    "    f\"Matches in all 3 tables: {complete_count}, Total unique matchids: {total_unique_matchids}\")\n",
    "\n",
    "# Rule I-06: Player count per match should be 22 (11 per team) +/- subs\n",
    "if player_col:\n",
    "    players_per_match = df_players.groupBy(\"matchid\").agg(\n",
    "        countDistinct(player_col).alias(\"player_count\")\n",
    "    )\n",
    "    valid_player_count = players_per_match.filter(\n",
    "        (col(\"player_count\") >= 20) & (col(\"player_count\") <= 30)\n",
    "    ).count()\n",
    "    total_matches_with_players = players_per_match.count()\n",
    "    log_dq_result(MATCH_PLAYERS_TABLE, \"consistency\", \"I-07: player_count_per_match\",\n",
    "        \"Each match should have 20-30 unique players (11 per team + possible subs)\",\n",
    "        total_matches_with_players, valid_player_count, 90.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9e46ad-10e9-4661-9ff8-bfc586089a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.5 VOLUME / STATISTICAL RULES\n",
    " _Checks row counts, distributions, and detects anomalies._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8d4e98-3f79-4b71-8326-49d194a9311c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: VOLUME / STATISTICAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rule S-01: match_events should have reasonable rows per match (100-300 for T20)\n",
    "events_per_match = df_events.groupBy(\"matchid\").count().withColumnRenamed(\"count\", \"row_count\")\n",
    "valid_volume = events_per_match.filter(\n",
    "    (col(\"row_count\") >= 100) & (col(\"row_count\") <= 300)\n",
    ").count()\n",
    "total_matches = events_per_match.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"volume\", \"S-01: events_per_match_reasonable\",\n",
    "    \"Each T20 match should have ~120-500 ball-by-ball events (including extras)\",\n",
    "    total_matches, valid_volume, 90.0)\n",
    "\n",
    "# Get stats for details\n",
    "stats = events_per_match.select(\n",
    "    _min(\"row_count\").alias(\"min_rows\"),\n",
    "    _max(\"row_count\").alias(\"max_rows\"),\n",
    "    avg(\"row_count\").alias(\"avg_rows\")\n",
    ").first()\n",
    "print(f\"  Events per match: min={stats['min_rows']}, max={stats['max_rows']}, avg={stats['avg_rows']:.0f}\")\n",
    "\n",
    "# Rule S-02: Each innings should have ~01-160 deliveries\n",
    "events_per_innings = df_events.groupBy(\"matchid\", \"innings\").count().withColumnRenamed(\"count\", \"row_count\")\n",
    "valid_innings_volume = events_per_innings.filter(\n",
    "    (col(\"row_count\") >= 1) & (col(\"row_count\") <= 160)\n",
    ").count()\n",
    "total_innings = events_per_innings.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"volume\", \"S-02: events_per_innings_reasonable\",\n",
    "    \"Each innings should have 30-200 deliveries (20 overs max + extras)\",\n",
    "    total_innings, valid_innings_volume, 90.0)\n",
    "\n",
    "# Rule S-03: Total row count should not be zero\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"volume\", \"S-03: events_table_not_empty\",\n",
    "    \"match_events table must contain data\",\n",
    "    1, 1 if events_count > 0 else 0, 100.0)\n",
    "\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"volume\", \"S-04: metadata_table_not_empty\",\n",
    "    \"match_metadata table must contain data\",\n",
    "    1, 1 if metadata_count > 0 else 0, 100.0)\n",
    "\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"volume\", \"S-05: players_table_not_empty\",\n",
    "    \"match_players table must contain data\",\n",
    "    1, 1 if players_count > 0 else 0, 100.0)\n",
    "\n",
    "# Rule S-06: Distinct bowlers per innings should be ~5-8 (T20)\n",
    "bowlers_per_innings = df_events.filter(col(\"bowler\").isNotNull()).groupBy(\"matchid\", \"innings\").agg(\n",
    "    countDistinct(\"bowler\").alias(\"bowler_count\")\n",
    ")\n",
    "valid_bowler_count = bowlers_per_innings.filter(\n",
    "    (col(\"bowler_count\") >= 3) & (col(\"bowler_count\") <= 10)\n",
    ").count()\n",
    "total_innings_bowler = bowlers_per_innings.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"volume\", \"S-06: bowlers_per_innings_reasonable\",\n",
    "    \"Each innings should have 3-10 distinct bowlers\",\n",
    "    total_innings_bowler, valid_bowler_count, 90.0)\n",
    "\n",
    "# Rule S-07: Distinct batsmen per innings should be ~2-11\n",
    "batsmen_per_innings = df_events.filter(col(\"batsman\").isNotNull()).groupBy(\"matchid\", \"innings\").agg(\n",
    "    countDistinct(\"batsman\").alias(\"batsman_count\")\n",
    ")\n",
    "valid_batsman_count = batsmen_per_innings.filter(\n",
    "    (col(\"batsman_count\") >= 2) & (col(\"batsman_count\") <= 11)\n",
    ").count()\n",
    "total_innings_batsman = batsmen_per_innings.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"volume\", \"S-07: batsmen_per_innings_reasonable\",\n",
    "    \"Each innings should have 2-11 distinct batsmen\",\n",
    "    total_innings_batsman, valid_batsman_count, 90.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "960eb623-b10b-4e01-a4e0-7e7425029d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.6 ACCURACY / DOMAIN-SPECIFIC RULES\n",
    "_Cricket-specific business logic validation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f455b08-7e8d-4a21-ba20-2441fed351ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: ACCURACY / DOMAIN-SPECIFIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rule A-01: First ball of each innings should be 0.1\n",
    "first_balls = df_events.groupBy(\"matchid\", \"innings\").agg(\n",
    "    _min(\"ball\").alias(\"first_ball\")\n",
    ")\n",
    "valid_first_ball = first_balls.filter(col(\"first_ball\") == \"0.1\").count()\n",
    "total_innings_first = first_balls.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"accuracy\", \"A-01: first_ball_is_0_1\",\n",
    "    \"The first delivery of each innings should be ball 0.1\",\n",
    "    total_innings_first, valid_first_ball, 95.0)\n",
    "\n",
    "# Rule A-02: Last over should not exceed 19 (0-indexed) in T20\n",
    "last_overs = df_events.withColumn(\n",
    "    \"over_num\", regexp_extract(col(\"ball\"), r\"^(\\d{1,2})\\.\", 1).cast(\"int\")\n",
    ").groupBy(\"matchid\", \"innings\").agg(\n",
    "    _max(\"over_num\").alias(\"last_over\")\n",
    ")\n",
    "valid_last_over = last_overs.filter(\n",
    "    (col(\"last_over\") >= 0) & (col(\"last_over\") <= 19)\n",
    ").count()\n",
    "total_innings_last = last_overs.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"accuracy\", \"A-02: max_over_within_T20_limit\",\n",
    "    \"Maximum over number should not exceed 19 (20 overs in T20, 0-indexed)\",\n",
    "    total_innings_last, valid_last_over, 95.0)\n",
    "\n",
    "\n",
    "# Rule A-03: Commentary text should have minimum length (not truncated)\n",
    "short_commentary = df_events.filter(\n",
    "    col(\"commentary\").isNotNull() & (length(trim(col(\"commentary\"))) < 5)\n",
    ").count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"accuracy\", \"A-03: commentary_min_length\",\n",
    "    \"Commentary text should be at least 5 characters (not truncated or garbage)\",\n",
    "    events_count, events_count - short_commentary, 75.0)\n",
    "\n",
    "# Rule A-04: No single bowler should bowl more than 4 overs in a T20 innings\n",
    "bowler_overs = df_events.filter(col(\"bowler\").isNotNull()).withColumn(\n",
    "    \"over_num\", regexp_extract(col(\"ball\"), r\"^(\\d{1,2})\\.\", 1).cast(\"int\")\n",
    ").groupBy(\"matchid\", \"innings\", \"bowler\").agg(\n",
    "    countDistinct(\"over_num\").alias(\"overs_bowled\")\n",
    ")\n",
    "over_limit_bowlers = bowler_overs.filter(col(\"overs_bowled\") > 4).count()\n",
    "total_bowler_spells = bowler_overs.count()\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"accuracy\", \"A-04: bowler_max_4_overs_t20\",\n",
    "    \"No bowler should bowl more than 4 overs in a T20 innings\",\n",
    "    total_bowler_spells, total_bowler_spells - over_limit_bowlers, 95.0,\n",
    "    f\"Bowler spells exceeding 4 overs: {over_limit_bowlers}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e9ae08-e865-48e0-9591-5a366e11bc69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.8 SCHEMA DRIFT DETECTION\n",
    "_Detects unexpected schema changes from Auto Loader._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7916213e-8ef3-45d8-befb-418fa2bf8635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rule SD-01: match_events expected columns\n",
    "expected_events_cols = {\"matchid\", \"ball\", \"innings\", \"event\", \"is_super_over\",\"commentary\", \n",
    "                         \"bowler\", \"batsman\", \"source_file\", \"load_timestamp\"}\n",
    "actual_events_cols = set(df_events.columns)\n",
    "missing_cols = expected_events_cols - actual_events_cols\n",
    "extra_cols = actual_events_cols - expected_events_cols - {\"_rescued_data\"}\n",
    "\n",
    "has_all_cols = 1 if len(missing_cols) == 0 else 0\n",
    "log_dq_result(MATCH_EVENTS_TABLE, \"schema\", \"SD-01: events_expected_columns\",\n",
    "    \"match_events should contain all expected columns\",\n",
    "    len(expected_events_cols), len(expected_events_cols) - len(missing_cols), 100.0,\n",
    "    f\"Missing: {missing_cols if missing_cols else 'None'}, Extra: {extra_cols if extra_cols else 'None'}\")\n",
    "\n",
    "# Rule SD-02: match_metadata expected columns\n",
    "expected_metadata_cols = {\"matchid\", \"ground\", \"series\", \"season\", \"toss\", \n",
    "                           \"player_of_the_match\", \"umpires\",\"tv_umpire\", \"source_file\", \"load_timestamp\"}\n",
    "actual_metadata_cols = set(df_metadata.columns)\n",
    "missing_m_cols = expected_metadata_cols - actual_metadata_cols\n",
    "\n",
    "log_dq_result(MATCH_METADATA_TABLE, \"schema\", \"SD-02: metadata_expected_columns\",\n",
    "    \"match_metadata should contain all expected columns\",\n",
    "    len(expected_metadata_cols), len(expected_metadata_cols) - len(missing_m_cols), 100.0,\n",
    "    f\"Missing: {missing_m_cols if missing_m_cols else 'None'}\")\n",
    "\n",
    "# Rule SD-03: match_players expected columns\n",
    "expected_players_cols = {\"matchid\", \"source_file\", \"load_timestamp\"}\n",
    "actual_players_cols = set(df_players.columns)\n",
    "missing_p_cols = expected_players_cols - actual_players_cols\n",
    "\n",
    "log_dq_result(MATCH_PLAYERS_TABLE, \"schema\", \"SD-03: players_expected_columns\",\n",
    "    \"match_players should contain all expected columns\",\n",
    "    len(expected_players_cols), len(expected_players_cols) - len(missing_p_cols), 100.0,\n",
    "    f\"Missing: {missing_p_cols if missing_p_cols else 'None'}\")\n",
    "\n",
    "# Rule SD-04: Check for _rescued_data (Auto Loader schema enforcement failures)\n",
    "for table_name, df, total in [\n",
    "    (MATCH_EVENTS_TABLE, df_events, events_count),\n",
    "    (MATCH_METADATA_TABLE, df_metadata, metadata_count),\n",
    "    (MATCH_PLAYERS_TABLE, df_players, players_count)\n",
    "]:\n",
    "    if \"_rescued_data\" in df.columns:\n",
    "        rescued_count = df.filter(col(\"_rescued_data\").isNotNull()).count()\n",
    "        log_dq_result(table_name, \"schema\", f\"SD-04: no_rescued_data_{table_name.split('.')[-1]}\",\n",
    "            \"Auto Loader _rescued_data column should be empty (no schema mismatches)\",\n",
    "            total, total - rescued_count, 99.0,\n",
    "            f\"Records with rescued data: {rescued_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82a306ed-1c67-40c9-bb3d-3f0c45ad2de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. DQ Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c686b2-987f-43ba-8200-274ce51c09a3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"table_name\":219,\"rule_name\":261},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770775891520}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"DATA QUALITY SUMMARY - Run ID: {run_id}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Query audit table for this run\n",
    "summary_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        status,\n",
    "        COUNT(*) as rule_count,\n",
    "        ROUND(AVG(pass_percentage), 2) as avg_pass_pct\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}'\n",
    "    GROUP BY status\n",
    "    ORDER BY status\n",
    "\"\"\")\n",
    "display(summary_df)\n",
    "\n",
    "# Detailed failures\n",
    "print(\"\\n--- FAILED RULES ---\")\n",
    "failures_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        table_name,\n",
    "        rule_category,\n",
    "        rule_name,\n",
    "        pass_percentage,\n",
    "        failed_records,\n",
    "        details\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}' AND status = 'FAIL'\n",
    "    ORDER BY pass_percentage ASC\n",
    "\"\"\")\n",
    "display(failures_df)\n",
    "\n",
    "# Warnings\n",
    "print(\"\\n--- WARNINGS ---\")\n",
    "warnings_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        table_name,\n",
    "        rule_category,\n",
    "        rule_name,\n",
    "        pass_percentage,\n",
    "        failed_records,\n",
    "        details\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}' AND status = 'WARN'\n",
    "    ORDER BY pass_percentage ASC\n",
    "\"\"\")\n",
    "display(warnings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "200ad8e2-1942-4af5-bb45-4e96b3a22685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Historical DQ Trend (Optional Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44439da4-3fca-41cb-a519-f7a43c6c0e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this to see DQ trends over time\n",
    "trend_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        run_id,\n",
    "        run_timestamp,\n",
    "        COUNT(*) as total_rules,\n",
    "        SUM(CASE WHEN status = 'PASS' THEN 1 ELSE 0 END) as passed,\n",
    "        SUM(CASE WHEN status = 'WARN' THEN 1 ELSE 0 END) as warnings,\n",
    "        SUM(CASE WHEN status = 'FAIL' THEN 1 ELSE 0 END) as failed,\n",
    "        ROUND(AVG(pass_percentage), 2) as avg_pass_pct\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    GROUP BY run_id, run_timestamp\n",
    "    ORDER BY run_timestamp DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "display(trend_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5630edfe-8930-4ae2-befe-ccef60828a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Pipeline Gate (Optional - Fail pipeline if critical rules fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "042abd06-2f8f-4126-9f9d-74c73d818bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for critical failures and optionally halt downstream processing\n",
    "critical_failures = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as cnt\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}' \n",
    "      AND status = 'FAIL'\n",
    "      AND threshold_pct = 100.0\n",
    "\"\"\").first()[\"cnt\"]\n",
    "\n",
    "if critical_failures > 0:\n",
    "    msg = f\"⛔ PIPELINE GATE: {critical_failures} critical DQ rule(s) failed! Review before proceeding to Silver layer.\"\n",
    "    print(msg)\n",
    "    # Uncomment to actually halt the pipeline:\n",
    "    # raise Exception(msg)\n",
    "else:\n",
    "    print(\"✅ PIPELINE GATE: All critical DQ rules passed. Safe to proceed to Silver layer.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8598518854948756,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DataQualityRulesBronzeLayer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
