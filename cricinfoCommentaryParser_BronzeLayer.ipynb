{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6093d16e-18bc-49cf-a5cc-19baeebf9041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f492ab4a-e3f9-459c-a8eb-7812ec4edef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, current_timestamp, col, lit\n",
    "from datetime import datetime,timezone\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b88ec1-2f98-4030-9518-818e36ea1991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d03866e-e7f0-430a-be4e-ca6613b96424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Job Parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Default values are used during interactive runs.\n",
    "# Databricks Job overrides these at runtime via the Parameters section.\n",
    "# Key names here must match exactly what the Job defines.\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"catalog_name\", \"T20_catalog_dev\",\n",
    "    \"Catalog Name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66f804ec-dcb8-466a-8c7d-1422fdcb30f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ADLS Storage Configuration\n",
    "storage_account = \"adlschitturidemo\"\n",
    "container = \"cricinfo-mens-international\"\n",
    "source_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/cricket_commentary/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb68e01-1c62-4682-8f3f-13818edf8f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unity Catalog Configuration\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "FULL_SCHEMA = f\"{CATALOG_NAME}.{SCHEMA_NAME}\"\n",
    "\n",
    "'''\n",
    "# Create catalog if it doesn't exists\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\n",
    "    MANAGED LOCATION 'abfss://unity-catalog-storage@dbstorage3x4najbqbebyi.dfs.core.windows.net/7405610025593193'\n",
    "\"\"\")\n",
    "'''\n",
    "\n",
    "# Create schema if it doesn't exists\n",
    "spark.sql(f\"\"\"CREATE SCHEMA IF NOT EXISTS {FULL_SCHEMA}\"\"\")\n",
    "\n",
    "#Create Unity Catalog Volume for pipeline files(Checkpoints & Schema)\n",
    "spark.sql(f\"\"\"CREATE VOLUME IF NOT EXISTS {FULL_SCHEMA}.pipeline_files\"\"\")\n",
    "\n",
    "# Unity Catalog Volume (works on serverless)\n",
    "checkpoint_base_path = f\"\"\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/pipeline_files/checkpoints/t20/\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb4013b-a65f-497d-8d85-457888f56c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create Tables under Bronze schema with CDC enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b30ca8e5-725d-466b-8358-feb48c4ddb7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {FULL_SCHEMA}.match_metadata (\n",
    "    Batchid STRING,\n",
    "    ground STRING,\n",
    "    toss STRING,\n",
    "    series STRING,\n",
    "    season STRING,\n",
    "    player_of_the_match STRING,\n",
    "    player_of_the_series STRING,\n",
    "    hours_of_play_local_time STRING,\n",
    "    match_days STRING,\n",
    "    t20_debut STRING,\n",
    "    t20i_debut STRING,\n",
    "    umpires STRING,\n",
    "    tv_umpire STRING,\n",
    "    reserve_umpire STRING,\n",
    "    match_referee STRING,\n",
    "    points STRING,\n",
    "    match_number STRING,\n",
    "    matchid BIGINT,\n",
    "    player_replacements STRING,\n",
    "    first_innings STRING,\n",
    "    second_innings STRING,\n",
    "    has_super_over BOOLEAN,\n",
    "    super_over_count INT,\n",
    "    series_result STRING,\n",
    "\n",
    "    -- Auto Loader metadata\n",
    "    load_timestamp TIMESTAMP,\n",
    "    source_file STRING\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (matchid)\n",
    "TBLPROPERTIES (\n",
    "  delta.enableChangeDataFeed = true\n",
    ")\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {FULL_SCHEMA}.match_events (\n",
    "    Batchid STRING,\n",
    "    match_ball_number BIGINT,\n",
    "    ball STRING,\n",
    "    event STRING,\n",
    "    score STRING,\n",
    "    commentary STRING,\n",
    "    bowler STRING,\n",
    "    batsman STRING,\n",
    "    innings STRING,\n",
    "    matchid BIGINT,\n",
    "    is_super_over BOOLEAN,\n",
    "\n",
    "    -- Auto Loader metadata\n",
    "    load_timestamp TIMESTAMP,\n",
    "    source_file STRING\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (matchid)\n",
    "TBLPROPERTIES (\n",
    "  delta.enableChangeDataFeed = true\n",
    ")\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {FULL_SCHEMA}.match_players (\n",
    "    Batchid STRING,\n",
    "    matchid BIGINT,\n",
    "    innings STRING,\n",
    "    team STRING,\n",
    "    player_name STRING,\n",
    "    batted BOOLEAN,\n",
    "    batting_position INT,\n",
    "    player_type STRING,\n",
    "    retired STRING,\n",
    "    not_out STRING,\n",
    "    bowled STRING,\n",
    "    \n",
    "    -- Auto Loader metadata\n",
    "    load_timestamp TIMESTAMP,\n",
    "    source_file STRING\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (matchid)\n",
    "TBLPROPERTIES (\n",
    "  delta.enableChangeDataFeed = true\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d058584b-cf13-4a99-b96d-4431140b7402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## IPL Data Ingestion pipeline for both the Incremental load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd93342-d231-47fd-823f-e9101246df1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IPLDataPipelineUnified:\n",
    "    \"\"\"Simplified pipeline using Auto Loader for both initial and incremental loads\"\"\"\n",
    "    \n",
    "    def __init__(self, source_base_path, catalog_name, schema_name, checkpoint_base_path):\n",
    "        self.source_base_path = source_base_path\n",
    "        self.catalog_name = catalog_name\n",
    "        self.schema_name = schema_name\n",
    "        self.full_schema = f\"{catalog_name}.{schema_name}\"\n",
    "        self.checkpoint_base_path = checkpoint_base_path\n",
    "        self.batch_id = self._generate_batch_id()\n",
    "        self.tables = {\n",
    "            \"match_events\": {\n",
    "                \"pattern\": \"match_events_data.csv\",\n",
    "                \"format\": \"csv\",\n",
    "                \"table_name\": f\"{self.full_schema}.match_events\",\n",
    "                \"description\": \"Ball-by-ball match events\",\n",
    "                \"schema_hints\": \"matchid BIGINT\"\n",
    "            },\n",
    "            \"match_metadata\": {\n",
    "                \"pattern\": \"metadata_data.json\",\n",
    "                \"format\": \"json\",\n",
    "                \"table_name\": f\"{self.full_schema}.match_metadata\",\n",
    "                \"description\": \"Match metadata and results\",\n",
    "                \"schema_hints\": \"matchid STRING\"\n",
    "            },\n",
    "            \"match_players\": {\n",
    "                \"pattern\": \"match_players_data.csv\",\n",
    "                \"format\": \"csv\",\n",
    "                \"table_name\": f\"{self.full_schema}.match_players\",\n",
    "                \"description\": \"Player information per match\",\n",
    "                \"schema_hints\": \"matchid BIGINT\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_batch_id():\n",
    "        \"\"\"Generate batch_id from Databricks job context or fallback to UUID.\"\"\"\n",
    "        try:\n",
    "            context = (dbutils.notebook.entry_point.getDbutils()\n",
    "                             .notebook().getContext())\n",
    "            job_id = context.jobId().get()\n",
    "            run_id = context.idInJob().get()\n",
    "            return f\"job_{job_id}_run_{run_id}\"\n",
    "        except Exception:\n",
    "            ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "            return f\"interactive_{ts}_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    def _add_metadata(self, df):\n",
    "        \"\"\"Extract matchid from _metadata.file_path\"\"\"\n",
    "        df = df.withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "                        .withColumn(\"source_file\",col(\"_metadata.file_path\")) \\\n",
    "                        .withColumn(\"Batchid\", lit(self.batch_id))\n",
    "        return df\n",
    "    \n",
    "    def load_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Unified load method using Auto Loader for both initial and incremental loads\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table to load\n",
    "            is_initial_load: If True, will overwrite table; if False, will append\n",
    "        \"\"\"\n",
    "        config = self.tables[table_name]\n",
    "        source_path = self.source_base_path\n",
    "        table_full_name = config['table_name']\n",
    "        checkpoint_path = f\"{self.checkpoint_base_path}{table_name}\"\n",
    "        schema_path = f\"{checkpoint_path}/schema\"\n",
    "        \n",
    "    \n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Target: {table_full_name}\")\n",
    "        print(f\"Format: {config['format']}\")\n",
    "        print(f\"Source: {source_path}\")\n",
    "        print(f\"Checkpoint: {checkpoint_path}\")\n",
    "        print(f\"Schema Hints: {config['schema_hints']}\")\n",
    "        print(f\"file format: {config['format']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Configure Auto Loader (works for both initial and incremental)\n",
    "        reader = spark.readStream.format(\"cloudFiles\") \\\n",
    "                    .option(\"cloudFiles.format\", config['format']) \\\n",
    "                    .option(\"cloudFiles.schemaLocation\", schema_path) \\\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "                    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "                    .option(\"cloudFiles.validateOptions\", \"true\") \\\n",
    "                    .option(\"cloudFiles.schemaHints\", config['schema_hints']) \\\n",
    "                    .option(\"pathGlobFilter\", config['pattern']) \\\n",
    "                    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "                    .option(\"cloudFiles.useNotifications\",   \"false\")\n",
    "        \n",
    "        # CSV-specific options\n",
    "        if config['format'] == 'csv':\n",
    "            reader = reader.option(\"header\", \"true\")\n",
    "        \n",
    "        # Load stream\n",
    "        df = reader.load(source_path) \\\n",
    "            .withColumn(\"matchid\", col(\"matchid\").cast(\"bigint\"))\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df = self._add_metadata(df)\n",
    "        \n",
    "        print(f\"Auto Loader configured successfully\")\n",
    "        \n",
    "        # Write stream with trigger(availableNow=True) for batch processing\n",
    "        query = df.writeStream \\\n",
    "            .format(\"delta\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .partitionBy(\"matchid\") \\\n",
    "            .trigger(availableNow=True) \\\n",
    "            .toTable(table_full_name)\n",
    "\n",
    "        \n",
    "        # Wait for completion\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        # Get statistics\n",
    "        count = spark.table(table_full_name).count()\n",
    "        partitions = spark.table(table_full_name).select(\"matchid\").distinct().count()\n",
    "        \n",
    "        print(f\"‚úì load completed\")\n",
    "        print(f\"‚úì Total rows: {count:,}\")\n",
    "        print(f\"‚úì Partitions: {partitions}\")\n",
    "\n",
    "        # Optimize after initial load\n",
    "        if not spark.catalog.tableExists(table_full_name):\n",
    "            self.optimize_table(table_name)\n",
    "        \n",
    "        return query\n",
    "    \n",
    "    def ingestion_auto_loader(self):\n",
    "        \"\"\"Load all tables using Auto Loader (incremental load)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INCREMENTAL LOAD - ALL TABLES (AUTO LOADER)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results = {}\n",
    "        for table_name in self.tables.keys():\n",
    "            try:\n",
    "                self.load_table(table_name)\n",
    "                results[table_name] = \"SUCCESS\"\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                results[table_name] = f\"FAILED: {e}\"\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INCREMENTAL LOAD SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        for table, status in results.items():\n",
    "            icon = \"‚úì\" if status == \"SUCCESS\" else \"‚úó\"\n",
    "            print(f\"{icon} {self.tables[table]['table_name']}: {status}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def optimize_table(self, table_name):\n",
    "        \"\"\"Optimize table with Z-Ordering\"\"\"\n",
    "        config = self.tables[table_name]\n",
    "        table_full_name = config['table_name']\n",
    "        \n",
    "        print(f\"\\nOptimizing {table_full_name}...\")\n",
    "        \n",
    "        # Z-Order on non-partition columns\n",
    "        if config.get('zorder_columns'):\n",
    "            cols = \", \".join(config['zorder_columns'])\n",
    "            print(f\"  Z-Ordering by: {cols}\")\n",
    "            spark.sql(f\"OPTIMIZE {table_full_name} ZORDER BY ({cols})\")\n",
    "        else:\n",
    "            spark.sql(f\"OPTIMIZE {table_full_name}\")\n",
    "        \n",
    "        print(f\"‚úì Optimization complete\")\n",
    "\n",
    "    def show_statistics(self):\n",
    "        \"\"\"Show statistics for all tables\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TABLE STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for table_name, config in self.tables.items():\n",
    "            table_full_name = config['table_name']\n",
    "            try:\n",
    "                df = spark.table(table_full_name)\n",
    "                count = df.count()\n",
    "                partitions = df.select(\"matchid\").distinct().count()\n",
    "                \n",
    "                print(f\"\\n{table_full_name}:\")\n",
    "                print(f\"  Format: {config['format']}\")\n",
    "                print(f\"  Rows: {count:,}\")\n",
    "                print(f\"  Partitions: {partitions}\")\n",
    "                \n",
    "                # Show sample\n",
    "                display(df.limit(3))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error: {e}\")\n",
    "    \n",
    "    def reset_checkpoint(self, table_name):\n",
    "        \"\"\"\n",
    "        Reset checkpoint to reprocess all files (use with caution!)\n",
    "        Useful for debugging or when you need to reload everything\n",
    "        \"\"\"\n",
    "        checkpoint_path = f\"{self.checkpoint_base_path}{table_name}\"\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è  WARNING: Resetting checkpoint for {table_name}\")\n",
    "        print(f\"   This will reprocess ALL files on next load\")\n",
    "        \n",
    "        try:\n",
    "            dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "            print(f\"‚úì Checkpoint reset: {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error resetting checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a8c2a16-96fe-44e0-bf47-62feacc06885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01eccb72-1c43-4da2-bff6-af15f08d4604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = IPLDataPipelineUnified(\n",
    "    source_base_path=source_base_path,\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    checkpoint_base_path=checkpoint_base_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a0c641-6039-4e63-985e-14dd0dcdbe86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTION 1: Incremental Load using Auto Loader\n",
    "print(\"üöÄ Running Incremental LOAD with Auto Loader...\")\n",
    "results = pipeline.ingestion_auto_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8e780b-c07a-43a9-b61b-10773899d888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show statistics\n",
    "pipeline.show_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1202276-b146-42da-a3e8-f4c033df80c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show all tables\n",
    "spark.sql(f\"SHOW TABLES IN {FULL_SCHEMA}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ee0ef6-a2e9-4036-8f82-918f30cdf923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query examples\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT matchid, COUNT(*) as row_count\n",
    "    FROM {FULL_SCHEMA}.match_events\n",
    "    GROUP BY matchid\n",
    "    ORDER BY matchid\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b3a58a-2544-47bb-908b-674423f87654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reset Checkpoint (Use Carefully!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ffac357-8678-4641-a02d-f61a39fb53de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to reset a checkpoint (will reprocess all files)\n",
    "#pipeline.reset_checkpoint(\"match_events\")\n",
    "#pipeline.reset_checkpoint(\"match_metadata\")\n",
    "#pipeline.reset_checkpoint(\"match_players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a815c7cc-2ff7-49ec-9e3e-167f5ce54e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run DQ checks after ingestion completes\n",
    "dq_result = dbutils.notebook.run(\n",
    "    \"/Workspace/Users/pradeepchitturi@gmail.com/Dev_T20CommentaryParser/DataQualityRulesBronzeLayer\",\n",
    "    timeout_seconds=600,  # 10 min timeout,\n",
    "    arguments = {\n",
    "        \"catalog_name\": CATALOG_NAME,\n",
    "        \"schema_name\":  SCHEMA_NAME\n",
    "    }\n",
    "\n",
    ")\n",
    "print(f\"DQ Result: {dq_result}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6948842457129843,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "cricinfoCommentaryParser_BronzeLayer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
