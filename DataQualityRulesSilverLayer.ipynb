{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73c2f8d-2315-4376-ba3b-73001b3f66de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Quality Rules — Silver Layer (Post-Transformation Validation)\n",
    "\n",
    "This notebook validates **Silver layer tables** after transformations applied by the\n",
    "`CricinfoCommentaryParser_SilverLayer` notebook.\n",
    "\n",
    "**Bronze DQ vs Silver DQ:**\n",
    "- Bronze DQ validates **raw ingested data** (nulls, format, schema drift from Auto Loader)\n",
    "- Silver DQ validates **derived columns, enrichments, and business logic** after transformations\n",
    "\n",
    "---\n",
    "\n",
    "### Silver Tables Validated\n",
    "\n",
    "| Silver Table | Merge Key | Transformations Validated |\n",
    "|---|---|---|\n",
    "| `silver.match_events` | matchid + match_ball_number + event | Runs calculation, dismissal extraction, name resolution, innings_score, wickets_lost, over/ball split |\n",
    "| `silver.match_players` | matchid + player_name + team | Deduplication, batting/bowling flags |\n",
    "| `silver.match_metadata` | matchid | Toss split, date parsing, timezone resolution, UTC conversion, team name resolution |\n",
    "\n",
    "### DQ Rule Categories\n",
    "\n",
    "| # | Category | Code Prefix | Purpose |\n",
    "|---|---|---|---|\n",
    "| 4.1 | Completeness | SC | Silver-derived columns must be populated |\n",
    "| 4.2 | Validity | SV | Runs, extras, dismissals, overs conform to expected domains/ranges |\n",
    "| 4.3 | Uniqueness | SU | Merge keys unique after dedup processing |\n",
    "| 4.4 | Consistency | SI | Cross-table referential integrity + internal logic |\n",
    "| 4.5 | Transformation Quality | ST | Name resolution, score accumulation, UTC conversion quality |\n",
    "| 4.6 | Accuracy | SA | Cricket domain rules on derived data |\n",
    "| 4.7 | Volume / Statistical | SS | Row counts, distributions, anomaly detection |\n",
    "| 4.7b | Bronze→Silver Row Count | SR | Cross-layer row count validation, data loss detection |\n",
    "| 4.8 | Schema | SD | Expected Silver columns present |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3d89221-b7cd-443f-864d-5db4eb858b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Tables Validated:**\n",
    "#### `T20_catalog.silver.match_events` — ball-by-ball with enriched runs, dismissals, name resolution, running totals\n",
    "#### `T20_catalog.silver.match_metadata` — match info with parsed dates, UTC times, split toss, resolved team names\n",
    "#### `T20_catalog.silver.match_players` — deduplicated player records per match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f03906e-5736-4ae7-95ee-8a752320d15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeac5612-be6d-4aee-924c-5262e27ed956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Job Parameters ────────────────────────────────────────────────────────────\n",
    "# Default values are used during interactive runs.\n",
    "# Databricks Job overrides these at runtime via the Parameters section.\n",
    "# Key names here must match exactly what the Job defines.\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"catalog_name\", \"T20_catalog\",\n",
    "    \"Catalog Name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efa6be62-0a0b-49fc-8e86-ae087f10dfb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, isnull, lit, sum as _sum, avg, min as _min, max as _max,\n",
    "    length, trim, regexp_extract, current_timestamp, countDistinct, expr,\n",
    "    upper, lower, abs as _abs, lag, lead, datediff, year, coalesce\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Unity Catalog Configuration\n",
    "CATALOG_NAME  = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA_NAME = \"silver\"\n",
    "FULL_SCHEMA = f\"{CATALOG_NAME}.{SCHEMA_NAME}\"\n",
    "\n",
    "# Silver DQ Audit table (separate from Bronze DQ)\n",
    "DQ_AUDIT_TABLE = f\"{FULL_SCHEMA}.dq_audit_log\"\n",
    "\n",
    "# Silver tables to validate\n",
    "SILVER_EVENTS   = f\"{FULL_SCHEMA}.match_events\"\n",
    "SILVER_METADATA = f\"{FULL_SCHEMA}.match_metadata\"\n",
    "SILVER_PLAYERS  = f\"{FULL_SCHEMA}.match_players\"\n",
    "\n",
    "# Run identifiers\n",
    "run_timestamp = datetime.now(timezone.utc)\n",
    "run_id = \"SLV_\" + run_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"DQ Run ID:     {run_id}\")\n",
    "print(f\"Run Timestamp: {run_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25415b6f-ff3d-4dae-b2d8-b684092c96db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. DQ Audit Table Setup & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8fd7a17-a368-415f-b703-483fd6b6872d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create audit table if not exists\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DQ_AUDIT_TABLE} (\n",
    "        run_id              STRING      COMMENT 'Unique identifier for this DQ run (SLV_ prefix for Silver)',\n",
    "        run_timestamp       TIMESTAMP   COMMENT 'When the DQ check was executed',\n",
    "        table_name          STRING      COMMENT 'Fully qualified table name being checked',\n",
    "        rule_category       STRING      COMMENT 'Category: completeness, validity, uniqueness, consistency, transformation, accuracy, volume, schema',\n",
    "        rule_name           STRING      COMMENT 'Unique rule identifier and descriptive name',\n",
    "        rule_description    STRING      COMMENT 'Detailed description of what the rule checks',\n",
    "        total_records       LONG        COMMENT 'Total records in scope',\n",
    "        passed_records      LONG        COMMENT 'Records that passed the check',\n",
    "        failed_records      LONG        COMMENT 'Records that failed the check',\n",
    "        pass_percentage     DOUBLE      COMMENT 'Percentage of records that passed',\n",
    "        status              STRING      COMMENT 'PASS, WARN, FAIL based on thresholds',\n",
    "        threshold_pct       DOUBLE      COMMENT 'Minimum acceptable pass percentage',\n",
    "        details             STRING      COMMENT 'Additional details or sample failures'\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Data quality audit log for Silver layer - IPL cricket pipeline'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ DQ audit table ready: {DQ_AUDIT_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db050bd6-4622-4b7b-a92f-fae2332deba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_dq_result(table_name, rule_category, rule_name, rule_description,\n",
    "                  total_records, passed_records, threshold_pct=100.0, details=\"\"):\n",
    "    \"\"\"Log a single DQ check result to the audit table.\"\"\"\n",
    "    \n",
    "    failed_records = total_records - passed_records\n",
    "    pass_pct = (passed_records / total_records * 100) if total_records > 0 else 0.0\n",
    "    \n",
    "    if pass_pct >= threshold_pct:\n",
    "        status = \"PASS\"\n",
    "    elif pass_pct >= (threshold_pct - 5):\n",
    "        status = \"WARN\"\n",
    "    else:\n",
    "        status = \"FAIL\"\n",
    "    \n",
    "    icon = \"✓\" if status == \"PASS\" else (\"⚠\" if status == \"WARN\" else \"✗\")\n",
    "    print(f\"  {icon} [{status}] {rule_name}: {pass_pct:.2f}% passed ({failed_records} failures)\")\n",
    "    \n",
    "    row = spark.createDataFrame([{\n",
    "        \"run_id\": run_id,\n",
    "        \"run_timestamp\": run_timestamp,\n",
    "        \"table_name\": table_name,\n",
    "        \"rule_category\": rule_category,\n",
    "        \"rule_name\": rule_name,\n",
    "        \"rule_description\": rule_description,\n",
    "        \"total_records\": int(total_records),\n",
    "        \"passed_records\": int(passed_records),\n",
    "        \"failed_records\": int(failed_records),\n",
    "        \"pass_percentage\": round(pass_pct, 2),\n",
    "        \"status\": status,\n",
    "        \"threshold_pct\": threshold_pct,\n",
    "        \"details\": details[:500]  # truncate long details\n",
    "    }])\n",
    "    \n",
    "    row.write.mode(\"append\").saveAsTable(DQ_AUDIT_TABLE)\n",
    "    return status\n",
    "\n",
    "\n",
    "def get_null_count(df, column):\n",
    "    \"\"\"Count nulls, empty strings, and 'null' string values.\"\"\"\n",
    "    return df.filter(\n",
    "        col(column).isNull() | \n",
    "        (trim(col(column)) == \"\") | \n",
    "        (lower(trim(col(column))) == \"null\") |\n",
    "        (lower(trim(col(column))) == \"none\") |\n",
    "        (lower(trim(col(column))) == \"n/a\")\n",
    "    ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e9dd06-abe8-4699-944d-faa7a5af4c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Load Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4a683e-edd1-4136-9d78-2a5acf7f265e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events   = spark.table(SILVER_EVENTS)\n",
    "df_metadata  = spark.table(SILVER_METADATA)\n",
    "df_players   = spark.table(SILVER_PLAYERS)\n",
    "\n",
    "events_count   = df_events.count()\n",
    "metadata_count = df_metadata.count()\n",
    "players_count  = df_players.count()\n",
    "\n",
    "print(f\"Silver Tables Loaded:\")\n",
    "print(f\"  {SILVER_EVENTS}:   {events_count:,} rows\")\n",
    "print(f\"  {SILVER_METADATA}: {metadata_count:,} rows\")\n",
    "print(f\"  {SILVER_PLAYERS}:  {players_count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7bf8790-745a-465e-a253-62ac1ec28666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. DATA QUALITY RULES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75ae644c-0d89-48e1-ac46-e10c0b73dba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 COMPLETENESS RULES\n",
    "_Checks that Silver-layer derived columns are populated (not null, not empty).\n",
    "These columns did not exist in Bronze — they were created by Silver transformations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b98add89-20d7-44fe-993a-2bb8c69b27b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: COMPLETENESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_EVENTS - Completeness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_EVENTS} ---\")\n",
    "\n",
    "# SC-E01: bowler (resolved full name) must not be null\n",
    "null_bowler = get_null_count(df_events, \"bowler\")\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E01: bowler_not_null\",\n",
    "    \"Bowler full name must be populated after name resolution UDF\",\n",
    "    events_count, events_count - null_bowler, 99.0)\n",
    "\n",
    "# SC-E02: batsman (resolved full name) must not be null\n",
    "null_batsman = get_null_count(df_events, \"batsman\")\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E02: batsman_not_null\",\n",
    "    \"Batsman full name must be populated after name resolution UDF\",\n",
    "    events_count, events_count - null_batsman, 99.0)\n",
    "\n",
    "# SC-E03: team must not be null (resolved from innings abbreviation)\n",
    "null_team = get_null_count(df_events, \"team\")\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E03: team_not_null\",\n",
    "    \"Team name must be resolved for every delivery (including Super Over backfill)\",\n",
    "    events_count, events_count - null_team, 99.0)\n",
    "\n",
    "# SC-E04: runs (calculated) must not be null\n",
    "null_runs = df_events.filter(col(\"runs\").isNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E04: runs_not_null\",\n",
    "    \"Calculated runs (from runs_text + no-ball penalty) must be populated for every delivery\",\n",
    "    events_count, events_count - null_runs, 100.0)\n",
    "\n",
    "# SC-E05: over (parsed from ball) must not be null\n",
    "null_over = df_events.filter(col(\"over\").isNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E05: over_not_null\",\n",
    "    \"Over number (1-indexed, parsed from ball column) must be populated\",\n",
    "    events_count, events_count - null_over, 100.0)\n",
    "\n",
    "# SC-E06: over_ball_number must not be null\n",
    "null_ball_num = df_events.filter(col(\"over_ball_number\").isNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E06: over_ball_number_not_null\",\n",
    "    \"Ball number within over (parsed from ball column) must be populated\",\n",
    "    events_count, events_count - null_ball_num, 100.0)\n",
    "\n",
    "# SC-E07: Extras classification must not be null\n",
    "null_extras = get_null_count(df_events, \"Extras\")\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E07: extras_not_null\",\n",
    "    \"Extras classification must be populated (normal, wide, noball, legbyes, byes)\",\n",
    "    events_count, events_count - null_extras, 100.0)\n",
    "\n",
    "# SC-E08: dismissal_method must not be null\n",
    "null_dismissal = get_null_count(df_events, \"dismissal_method\")\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E08: dismissal_method_not_null\",\n",
    "    \"Dismissal method must be populated ('Not Out' when no wicket falls)\",\n",
    "    events_count, events_count - null_dismissal, 100.0)\n",
    "\n",
    "# SC-E09: innings_score (running total) must not be null\n",
    "null_innings_score = df_events.filter(col(\"innings_score\").isNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E09: innings_score_not_null\",\n",
    "    \"Running innings score (cumulative sum of runs) must be calculated for every delivery\",\n",
    "    events_count, events_count - null_innings_score, 100.0)\n",
    "\n",
    "# SC-E10: wickets_lost (running total) must not be null\n",
    "null_wickets = df_events.filter(col(\"wickets_lost\").isNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E10: wickets_lost_not_null\",\n",
    "    \"Running wickets count must be calculated for every delivery\",\n",
    "    events_count, events_count - null_wickets, 100.0)\n",
    "\n",
    "# SC-E11: silver_load_timestamp must not be null\n",
    "null_silver_ts = df_events.filter(col(\"silver_load_timestamp\").isNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"completeness\", \"SC-E11: silver_load_timestamp_not_null\",\n",
    "    \"Silver processing timestamp must be present for lineage tracking\",\n",
    "    events_count, events_count - null_silver_ts, 100.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_METADATA - Completeness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_METADATA} ---\")\n",
    "\n",
    "# SC-M01: match_date (parsed from match_days) must not be null\n",
    "null_match_date = df_metadata.filter(col(\"match_date\").isNull()).count()\n",
    "log_dq_result(SILVER_METADATA, \"completeness\", \"SC-M01: match_date_not_null\",\n",
    "    \"Parsed match_date (from 'd MMMM yyyy' in match_days) must be populated\",\n",
    "    metadata_count, metadata_count - null_match_date, 100.0)\n",
    "\n",
    "# SC-M02: toss (split toss winner) must not be null\n",
    "null_toss = get_null_count(df_metadata, \"toss\")\n",
    "log_dq_result(SILVER_METADATA, \"completeness\", \"SC-M02: toss_winner_not_null\",\n",
    "    \"Toss winner team name must be populated after splitting toss column\",\n",
    "    metadata_count, metadata_count - null_toss, 100.0)\n",
    "\n",
    "# SC-M03: decision (split toss decision) must not be null\n",
    "null_decision = get_null_count(df_metadata, \"decision\")\n",
    "log_dq_result(SILVER_METADATA, \"completeness\", \"SC-M03: decision_not_null\",\n",
    "    \"Toss decision ('elected to bat/field first') must be populated after split\",\n",
    "    metadata_count, metadata_count - null_decision, 95.0)\n",
    "\n",
    "# SC-M04: match_start_utc must not be null\n",
    "null_start_utc = df_metadata.filter(col(\"match_start_utc\").isNull()).count()\n",
    "log_dq_result(SILVER_METADATA, \"completeness\", \"SC-M04: match_start_utc_not_null\",\n",
    "    \"UTC match start time (geocoded timezone → to_utc_timestamp) must be populated\",\n",
    "    metadata_count, metadata_count - null_start_utc, 90.0)\n",
    "\n",
    "# SC-M05: first_innings (resolved team name) must not be null\n",
    "null_first = get_null_count(df_metadata, \"first_innings\")\n",
    "log_dq_result(SILVER_METADATA, \"completeness\", \"SC-M05: first_innings_not_null\",\n",
    "    \"First innings team name must be resolved via get_full_name UDF\",\n",
    "    metadata_count, metadata_count - null_first, 95.0)\n",
    "\n",
    "# SC-M06: second_innings (resolved team name) must not be null\n",
    "null_second = get_null_count(df_metadata, \"second_innings\")\n",
    "log_dq_result(SILVER_METADATA, \"completeness\", \"SC-M06: second_innings_not_null\",\n",
    "    \"Second innings team name must be resolved via get_full_name UDF\",\n",
    "    metadata_count, metadata_count - null_second, 95.0)\n",
    "\n",
    "# SC-M07: silver_load_timestamp must not be null\n",
    "null_m_silver_ts = df_metadata.filter(col(\"silver_load_timestamp\").isNull()).count()\n",
    "log_dq_result(SILVER_METADATA, \"completeness\", \"SC-M07: silver_load_timestamp_not_null\",\n",
    "    \"Silver processing timestamp must be present for lineage tracking\",\n",
    "    metadata_count, metadata_count - null_m_silver_ts, 100.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_PLAYERS - Completeness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_PLAYERS} ---\")\n",
    "\n",
    "# SC-P01: player_name must not be null\n",
    "null_player = get_null_count(df_players, \"player_name\")\n",
    "log_dq_result(SILVER_PLAYERS, \"completeness\", \"SC-P01: player_name_not_null\",\n",
    "    \"Player name must be populated in Silver after deduplication\",\n",
    "    players_count, players_count - null_player, 100.0)\n",
    "\n",
    "# SC-P02: team must not be null\n",
    "null_p_team = get_null_count(df_players, \"team\")\n",
    "log_dq_result(SILVER_PLAYERS, \"completeness\", \"SC-P02: team_not_null\",\n",
    "    \"Team name must be populated for every player record\",\n",
    "    players_count, players_count - null_p_team, 100.0)\n",
    "\n",
    "# SC-P03: innings must not be null\n",
    "null_p_innings = get_null_count(df_players, \"innings\")\n",
    "log_dq_result(SILVER_PLAYERS, \"completeness\", \"SC-P03: innings_not_null\",\n",
    "    \"Innings assignment must be populated for every player\",\n",
    "    players_count, players_count - null_p_innings, 100.0)\n",
    "\n",
    "# SC-P04: silver_load_timestamp must not be null\n",
    "null_p_silver_ts = df_players.filter(col(\"silver_load_timestamp\").isNull()).count()\n",
    "log_dq_result(SILVER_PLAYERS, \"completeness\", \"SC-P04: silver_load_timestamp_not_null\",\n",
    "    \"Silver processing timestamp must be present for lineage tracking\",\n",
    "    players_count, players_count - null_p_silver_ts, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2b6cd0-9d45-47d5-91f7-d84af890d71e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2 VALIDITY / FORMAT RULES\n",
    "_Checks that Silver-derived values conform to expected ranges and domains._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffdf6d7b-2449-406a-bdde-2ae1490d8417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: VALIDITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_EVENTS - Validity\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_EVENTS} ---\")\n",
    "\n",
    "# SV-E01: runs must be non-negative integer 0-7 (max: 6 + 1 no-ball penalty)\n",
    "invalid_runs = df_events.filter(\n",
    "    (col(\"runs\").isNull()) | (col(\"runs\") < 0) | (col(\"runs\") > 7)\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E01: runs_range_0_7\",\n",
    "    \"Calculated runs per ball must be 0-7 (max: 6 boundary + 1 no-ball penalty)\",\n",
    "    events_count, events_count - invalid_runs, 100.0)\n",
    "\n",
    "# SV-E02: over must be 1-20 (1-indexed) for non-Super Over deliveries\n",
    "invalid_over = df_events.filter(\n",
    "    (col(\"over\").isNotNull()) &\n",
    "    ((col(\"over\") < 1) | (col(\"over\") > 20)) &\n",
    "    (col(\"is_super_over\") == False)\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E02: over_range_1_20\",\n",
    "    \"Over number must be 1-20 for regular T20 innings (1-indexed from ball split)\",\n",
    "    events_count, events_count - invalid_over, 98.0)\n",
    "\n",
    "# SV-E03: over_ball_number must be 1-6\n",
    "invalid_ball_num = df_events.filter(\n",
    "    (col(\"over_ball_number\").isNotNull()) &\n",
    "    ((col(\"over_ball_number\") < 1) | (col(\"over_ball_number\") > 6))\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E03: ball_number_range_1_6\",\n",
    "    \"Ball number within over must be 1-6 (parsed from decimal part of ball)\",\n",
    "    events_count, events_count - invalid_ball_num, 98.0)\n",
    "\n",
    "# SV-E04: Extras must be a valid domain value\n",
    "valid_extras = [\"normal\", \"wide\", \"noball\", \"legbyes\", \"byes\"]\n",
    "invalid_extras = df_events.filter(\n",
    "    col(\"Extras\").isNotNull() & ~col(\"Extras\").isin(valid_extras)\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E04: extras_valid_domain\",\n",
    "    f\"Extras must be one of: {', '.join(valid_extras)}\",\n",
    "    events_count, events_count - invalid_extras, 100.0)\n",
    "\n",
    "# SV-E05: dismissal_method must be a valid domain value\n",
    "valid_dismissals = [\"Not Out\", \"Run Out\", \"Caught\", \"Caught & Bowled\", \"LBW\", \"Stumped\", \"Bowled\"]\n",
    "invalid_dismissal = df_events.filter(\n",
    "    col(\"dismissal_method\").isNotNull() & ~col(\"dismissal_method\").isin(valid_dismissals)\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E05: dismissal_method_valid_domain\",\n",
    "    f\"Dismissal method must be one of: {', '.join(valid_dismissals)}\",\n",
    "    events_count, events_count - invalid_dismissal, 100.0)\n",
    "\n",
    "# SV-E06: innings_score must be non-negative\n",
    "negative_score = df_events.filter(col(\"innings_score\") < 0).count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E06: innings_score_non_negative\",\n",
    "    \"Running innings score (cumulative sum) must be >= 0\",\n",
    "    events_count, events_count - negative_score, 100.0)\n",
    "\n",
    "# SV-E07: wickets_lost must be 0-10\n",
    "invalid_wickets = df_events.filter(\n",
    "    (col(\"wickets_lost\") < 0) | (col(\"wickets_lost\") > 10)\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E07: wickets_lost_range_0_10\",\n",
    "    \"Wickets lost per innings must be between 0 and 10\",\n",
    "    events_count, events_count - invalid_wickets, 100.0)\n",
    "\n",
    "# SV-E08: super_over_team must be 'normal' or contain 'Super Over'\n",
    "invalid_so_team = df_events.filter(\n",
    "    (col(\"super_over_team\") != \"normal\") &\n",
    "    (~col(\"super_over_team\").contains(\"Super Over\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E08: super_over_team_valid\",\n",
    "    \"super_over_team must be 'normal' or contain 'Super Over'\",\n",
    "    events_count, events_count - invalid_so_team, 100.0)\n",
    "\n",
    "# SV-E09: bowler name should be alphabetic (resolved names)\n",
    "invalid_bowler_fmt = df_events.filter(\n",
    "    col(\"bowler\").isNotNull() & ~col(\"bowler\").rlike(r\"^[A-Za-z\\s\\.\\'\\-]+$\")\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E09: bowler_name_format\",\n",
    "    \"Resolved bowler name should contain only letters, spaces, dots, hyphens, apostrophes\",\n",
    "    events_count, events_count - invalid_bowler_fmt, 95.0)\n",
    "\n",
    "# SV-E10: batsman name should be alphabetic (resolved names)\n",
    "invalid_batsman_fmt = df_events.filter(\n",
    "    col(\"batsman\").isNotNull() & ~col(\"batsman\").rlike(r\"^[A-Za-z\\s\\.\\'\\-]+$\")\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"validity\", \"SV-E10: batsman_name_format\",\n",
    "    \"Resolved batsman name should contain only letters, spaces, dots, hyphens, apostrophes\",\n",
    "    events_count, events_count - invalid_batsman_fmt, 95.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_METADATA - Validity\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_METADATA} ---\")\n",
    "\n",
    "# SV-M01: match_date year must be 2008-2030\n",
    "invalid_date = df_metadata.filter(\n",
    "    col(\"match_date\").isNotNull() &\n",
    "    ((year(col(\"match_date\")) < 2008) | (year(col(\"match_date\")) > 2030))\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"validity\", \"SV-M01: match_date_valid_range\",\n",
    "    \"Parsed match_date year must be between 2008 and 2030\",\n",
    "    metadata_count, metadata_count - invalid_date, 100.0)\n",
    "\n",
    "# SV-M02: decision must contain 'bat' or 'field'\n",
    "invalid_decision = df_metadata.filter(\n",
    "    col(\"decision\").isNotNull() &\n",
    "    (~lower(col(\"decision\")).contains(\"bat\")) &\n",
    "    (~lower(col(\"decision\")).contains(\"field\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"validity\", \"SV-M02: decision_bat_or_field\",\n",
    "    \"Toss decision (split from toss column) must contain 'bat' or 'field'\",\n",
    "    metadata_count, metadata_count - invalid_decision, 95.0)\n",
    "\n",
    "# SV-M03: first_innings != second_innings (different teams)\n",
    "same_teams = df_metadata.filter(\n",
    "    col(\"first_innings\").isNotNull() &\n",
    "    col(\"second_innings\").isNotNull() &\n",
    "    (trim(col(\"first_innings\")) == trim(col(\"second_innings\")))\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"validity\", \"SV-M03: innings_teams_different\",\n",
    "    \"First and second innings must have different resolved team names\",\n",
    "    metadata_count, metadata_count - same_teams, 100.0)\n",
    "\n",
    "# SV-M04: match_start_utc should be within ±1 day of match_date (timezone offset)\n",
    "invalid_utc = df_metadata.filter(\n",
    "    col(\"match_start_utc\").isNotNull() &\n",
    "    col(\"match_date\").isNotNull() &\n",
    "    (_abs(datediff(col(\"match_start_utc\").cast(\"date\"), col(\"match_date\"))) > 1)\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"validity\", \"SV-M04: utc_matches_match_date\",\n",
    "    \"match_start_utc date should be within 1 day of match_date (timezone offset variance)\",\n",
    "    metadata_count, metadata_count - invalid_utc, 95.0)\n",
    "\n",
    "# SV-M05: first_innings_start_utc < first_innings_end_utc (session order)\n",
    "invalid_session_order = df_metadata.filter(\n",
    "    col(\"first_innings_start_utc\").isNotNull() &\n",
    "    col(\"first_innings_end_utc\").isNotNull() &\n",
    "    (col(\"first_innings_start_utc\") >= col(\"first_innings_end_utc\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"validity\", \"SV-M05: first_session_times_ordered\",\n",
    "    \"First innings start UTC must be before first innings end UTC\",\n",
    "    metadata_count, metadata_count - invalid_session_order, 100.0)\n",
    "\n",
    "# SV-M06: second_innings_start_utc < second_innings_end_utc\n",
    "invalid_session2 = df_metadata.filter(\n",
    "    col(\"second_innings_start_utc\").isNotNull() &\n",
    "    col(\"second_innings_end_utc\").isNotNull() &\n",
    "    (col(\"second_innings_start_utc\") >= col(\"second_innings_end_utc\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"validity\", \"SV-M06: second_session_times_ordered\",\n",
    "    \"Second innings start UTC must be before second innings end UTC\",\n",
    "    metadata_count, metadata_count - invalid_session2, 100.0)\n",
    "\n",
    "# SV-M07: super_over_count must be non-negative\n",
    "if \"super_over_count\" in df_metadata.columns:\n",
    "    invalid_so_count = df_metadata.filter(\n",
    "        col(\"super_over_count\").isNotNull() & (col(\"super_over_count\") < 0)\n",
    "    ).count()\n",
    "    log_dq_result(SILVER_METADATA, \"validity\", \"SV-M07: super_over_count_non_negative\",\n",
    "        \"super_over_count must be >= 0\",\n",
    "        metadata_count, metadata_count - invalid_so_count, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d599739-95c9-4110-b1ba-4c0bab3d0930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3 UNIQUENESS RULES\n",
    "_Checks that Silver merge keys are unique — no duplicates should remain after dedup_dataframe processing._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d4c1462-a672-4038-bf88-5fa60e671c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: UNIQUENESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_EVENTS - Uniqueness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_EVENTS} ---\")\n",
    "\n",
    "# SU-E01: Merge key (matchid + match_ball_number + event) must be unique\n",
    "distinct_event_keys = df_events.select(\"matchid\", \"match_ball_number\", \"event\").distinct().count()\n",
    "log_dq_result(SILVER_EVENTS, \"uniqueness\", \"SU-E01: merge_key_unique\",\n",
    "    \"Merge key (matchid + match_ball_number + event) must be unique in Silver after dedup\",\n",
    "    events_count, distinct_event_keys, 100.0,\n",
    "    f\"Total rows: {events_count}, Distinct keys: {distinct_event_keys}\")\n",
    "\n",
    "# SU-E02: No fully duplicate rows\n",
    "full_distinct_e = df_events.distinct().count()\n",
    "log_dq_result(SILVER_EVENTS, \"uniqueness\", \"SU-E02: no_full_duplicates\",\n",
    "    \"There should be no fully identical rows across all columns\",\n",
    "    events_count, full_distinct_e, 100.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_METADATA - Uniqueness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_METADATA} ---\")\n",
    "\n",
    "# SU-M01: matchid should be unique (one metadata record per match)\n",
    "distinct_m_matchid = df_metadata.select(\"matchid\").distinct().count()\n",
    "log_dq_result(SILVER_METADATA, \"uniqueness\", \"SU-M01: matchid_unique\",\n",
    "    \"Each match should have exactly one Silver metadata record after dedup\",\n",
    "    metadata_count, distinct_m_matchid, 100.0,\n",
    "    f\"Total rows: {metadata_count}, Distinct matchids: {distinct_m_matchid}\")\n",
    "\n",
    "# SU-M02: No fully duplicate metadata rows\n",
    "full_distinct_m = df_metadata.distinct().count()\n",
    "log_dq_result(SILVER_METADATA, \"uniqueness\", \"SU-M02: no_full_duplicates\",\n",
    "    \"There should be no fully identical metadata rows\",\n",
    "    metadata_count, full_distinct_m, 100.0)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SILVER MATCH_PLAYERS - Uniqueness\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- {SILVER_PLAYERS} ---\")\n",
    "\n",
    "# SU-P01: Merge key (matchid + player_name + team) must be unique\n",
    "distinct_player_keys = df_players.select(\"matchid\", \"player_name\", \"team\").distinct().count()\n",
    "log_dq_result(SILVER_PLAYERS, \"uniqueness\", \"SU-P01: merge_key_unique\",\n",
    "    \"Merge key (matchid + player_name + team) must be unique in Silver after dedup\",\n",
    "    players_count, distinct_player_keys, 100.0,\n",
    "    f\"Total rows: {players_count}, Distinct keys: {distinct_player_keys}\")\n",
    "\n",
    "# SU-P02: No fully duplicate player rows\n",
    "full_distinct_p = df_players.distinct().count()\n",
    "log_dq_result(SILVER_PLAYERS, \"uniqueness\", \"SU-P02: no_full_duplicates\",\n",
    "    \"There should be no fully identical player rows after dedup\",\n",
    "    players_count, full_distinct_p, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf901b50-5642-4045-ae0e-45408fcf53fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.4 CONSISTENCY / CROSS-TABLE INTEGRITY RULES\n",
    "_Checks referential integrity across Silver tables and internal logical consistency._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e403f82-9431-481f-a547-6abfa07ec8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: CONSISTENCY / INTEGRITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "events_matchids  = df_events.select(\"matchid\").distinct()\n",
    "metadata_matchids = df_metadata.select(\"matchid\").distinct()\n",
    "players_matchids  = df_players.select(\"matchid\").distinct()\n",
    "\n",
    "# SI-01: Every event matchid must exist in metadata\n",
    "orphan_events = events_matchids.join(metadata_matchids, \"matchid\", \"left_anti\").count()\n",
    "total_event_matchids = events_matchids.count()\n",
    "log_dq_result(SILVER_EVENTS, \"integrity\", \"SI-01: events_matchid_in_metadata\",\n",
    "    \"Every matchid in Silver events must have a corresponding Silver metadata record\",\n",
    "    total_event_matchids, total_event_matchids - orphan_events, 100.0,\n",
    "    f\"Orphan event matchids (no metadata): {orphan_events}\")\n",
    "\n",
    "# SI-02: Every event matchid must exist in players\n",
    "orphan_events_p = events_matchids.join(players_matchids, \"matchid\", \"left_anti\").count()\n",
    "log_dq_result(SILVER_EVENTS, \"integrity\", \"SI-02: events_matchid_in_players\",\n",
    "    \"Every matchid in Silver events must have Silver player records\",\n",
    "    total_event_matchids, total_event_matchids - orphan_events_p, 100.0,\n",
    "    f\"Orphan event matchids (no players): {orphan_events_p}\")\n",
    "\n",
    "# SI-03: Every metadata matchid should have events\n",
    "total_meta_matchids = metadata_matchids.count()\n",
    "metadata_no_events = metadata_matchids.join(events_matchids, \"matchid\", \"left_anti\").count()\n",
    "log_dq_result(SILVER_METADATA, \"integrity\", \"SI-03: metadata_has_events\",\n",
    "    \"Every match in Silver metadata should have ball-by-ball events\",\n",
    "    total_meta_matchids, total_meta_matchids - metadata_no_events, 95.0,\n",
    "    f\"Matches with metadata but no events: {metadata_no_events}\")\n",
    "\n",
    "# SI-04: matchid sets aligned across all 3 Silver tables\n",
    "all_three = events_matchids.join(metadata_matchids, \"matchid\", \"inner\") \\\n",
    "    .join(players_matchids, \"matchid\", \"inner\").count()\n",
    "all_unique = events_matchids.union(metadata_matchids).union(players_matchids).distinct().count()\n",
    "log_dq_result(SILVER_EVENTS, \"integrity\", \"SI-04: all_tables_matchid_aligned\",\n",
    "    \"All three Silver tables should have the same set of matchids\",\n",
    "    all_unique, all_three, 95.0,\n",
    "    f\"In all 3 tables: {all_three}, Total unique: {all_unique}\")\n",
    "\n",
    "# SI-05: Bowler names in events should exist in players table\n",
    "event_bowlers = df_events.select(\"matchid\", col(\"bowler\").alias(\"player_name\")).distinct()\n",
    "player_names  = df_players.select(\"matchid\", \"player_name\").distinct()\n",
    "orphan_bowlers = event_bowlers.join(player_names, [\"matchid\", \"player_name\"], \"left_anti\").count()\n",
    "total_bowler_entries = event_bowlers.count()\n",
    "log_dq_result(SILVER_EVENTS, \"integrity\", \"SI-05: bowlers_in_players_table\",\n",
    "    \"Every bowler in Silver events should exist in Silver players for the same match\",\n",
    "    total_bowler_entries, total_bowler_entries - orphan_bowlers, 90.0,\n",
    "    f\"Bowler-match combos not found in players: {orphan_bowlers}\")\n",
    "\n",
    "# SI-06: Batsman names in events should exist in players table\n",
    "event_batsmen = df_events.select(\"matchid\", col(\"batsman\").alias(\"player_name\")).distinct()\n",
    "orphan_batsmen = event_batsmen.join(player_names, [\"matchid\", \"player_name\"], \"left_anti\").count()\n",
    "total_batsman_entries = event_batsmen.count()\n",
    "log_dq_result(SILVER_EVENTS, \"integrity\", \"SI-06: batsmen_in_players_table\",\n",
    "    \"Every batsman in Silver events should exist in Silver players for the same match\",\n",
    "    total_batsman_entries, total_batsman_entries - orphan_batsmen, 90.0,\n",
    "    f\"Batsman-match combos not found in players: {orphan_batsmen}\")\n",
    "\n",
    "# SI-07: Dismissals must have dismissed_by populated\n",
    "dismissals = df_events.filter(col(\"dismissal_method\") != \"Not Out\")\n",
    "dismissals_count = dismissals.count()\n",
    "dismissal_no_fielder = dismissals.filter(\n",
    "    col(\"dismissed_by\").isNull() | (trim(col(\"dismissed_by\")) == \"\")\n",
    ").count()\n",
    "if dismissals_count > 0:\n",
    "    log_dq_result(SILVER_EVENTS, \"consistency\", \"SI-07: dismissal_has_fielder\",\n",
    "        \"Wickets (dismissal_method != Not Out) should have dismissed_by populated\",\n",
    "        dismissals_count, dismissals_count - dismissal_no_fielder, 90.0,\n",
    "        f\"Dismissals missing dismissed_by: {dismissal_no_fielder}\")\n",
    "\n",
    "# SI-08: Not Out rows should NOT have dismissed_by\n",
    "not_outs = df_events.filter(col(\"dismissal_method\") == \"Not Out\")\n",
    "not_outs_count = not_outs.count()\n",
    "fielder_on_not_out = not_outs.filter(\n",
    "    col(\"dismissed_by\").isNotNull() & (trim(col(\"dismissed_by\")) != \"\")\n",
    ").count()\n",
    "if not_outs_count > 0:\n",
    "    log_dq_result(SILVER_EVENTS, \"consistency\", \"SI-08: not_out_no_fielder\",\n",
    "        \"'Not Out' deliveries should not have dismissed_by populated\",\n",
    "        not_outs_count, not_outs_count - fielder_on_not_out, 100.0)\n",
    "\n",
    "# SI-09: Player count per match should be 20-30\n",
    "players_per_match = df_players.groupBy(\"matchid\").agg(\n",
    "    countDistinct(\"player_name\").alias(\"player_count\")\n",
    ")\n",
    "valid_player_count = players_per_match.filter(\n",
    "    (col(\"player_count\") >= 20) & (col(\"player_count\") <= 30)\n",
    ").count()\n",
    "total_matches_p = players_per_match.count()\n",
    "log_dq_result(SILVER_PLAYERS, \"consistency\", \"SI-09: player_count_per_match\",\n",
    "    \"Each match should have 20-30 unique players (11 per team + possible subs)\",\n",
    "    total_matches_p, valid_player_count, 90.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "891a0f41-73f5-4fc8-aece-f95ccb8c5df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.5 TRANSFORMATION QUALITY RULES\n",
    "_Validates Silver-specific transformations: name resolution quality, score accumulation integrity, UTC conversion correctness._\n",
    "\n",
    "These rules have **no Bronze equivalent** — they validate the quality of Silver transformations themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cfd6c3a-68b9-493e-adb2-5841956a34ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: TRANSFORMATION QUALITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# Name Resolution Quality\n",
    "# ──────────────────────────────────────────────\n",
    "print(\"\\n--- Name Resolution ---\")\n",
    "\n",
    "# ST-01: Bowler names should be fully resolved (contain space → first + last name)\n",
    "unresolved_bowler = df_events.filter(\n",
    "    col(\"bowler\").isNotNull() & (~col(\"bowler\").contains(\" \"))\n",
    ").count()\n",
    "non_null_bowler = df_events.filter(col(\"bowler\").isNotNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"transformation\", \"ST-01: bowler_name_resolved\",\n",
    "    \"Bowler names should be full names (contain at least one space = first + last)\",\n",
    "    non_null_bowler, non_null_bowler - unresolved_bowler, 95.0,\n",
    "    f\"Unresolved bowlers (single word, no space): {unresolved_bowler}\")\n",
    "\n",
    "# ST-02: Batsman names should be fully resolved\n",
    "unresolved_batsman = df_events.filter(\n",
    "    col(\"batsman\").isNotNull() & (~col(\"batsman\").contains(\" \"))\n",
    ").count()\n",
    "non_null_batsman = df_events.filter(col(\"batsman\").isNotNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"transformation\", \"ST-02: batsman_name_resolved\",\n",
    "    \"Batsman names should be full names (contain at least one space)\",\n",
    "    non_null_batsman, non_null_batsman - unresolved_batsman, 95.0,\n",
    "    f\"Unresolved batsmen (single word, no space): {unresolved_batsman}\")\n",
    "\n",
    "# ST-03: Team names should be full names (not abbreviations ≤3 chars like SA, AFG)\n",
    "short_team = df_events.filter(\n",
    "    col(\"team\").isNotNull() & (length(col(\"team\")) <= 3)\n",
    ").count()\n",
    "non_null_team = df_events.filter(col(\"team\").isNotNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"transformation\", \"ST-03: team_name_resolved\",\n",
    "    \"Team names should be full names (>3 chars, not abbreviations)\",\n",
    "    non_null_team, non_null_team - short_team, 95.0,\n",
    "    f\"Abbreviated teams still present (<=3 chars): {short_team}\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# Score Accumulation Integrity\n",
    "# ──────────────────────────────────────────────\n",
    "print(\"\\n--- Score Accumulation ---\")\n",
    "\n",
    "# Shared window for innings ordering\n",
    "score_window = Window.partitionBy(\"matchid\", \"team\", \"super_over_team\") \\\n",
    "    .orderBy(\"over\", \"over_ball_number\")\n",
    "df_score_check = df_events.withColumn(\"prev_score\", lag(\"innings_score\").over(score_window))\n",
    "\n",
    "# ST-04: innings_score must never decrease (monotonically non-decreasing)\n",
    "decreasing_score = df_score_check.filter(\n",
    "    col(\"prev_score\").isNotNull() & (col(\"innings_score\") < col(\"prev_score\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"transformation\", \"ST-04: innings_score_monotonic\",\n",
    "    \"Running innings score must never decrease within an innings\",\n",
    "    events_count, events_count - decreasing_score, 100.0,\n",
    "    f\"Score decreases detected: {decreasing_score}\")\n",
    "\n",
    "# ST-05: wickets_lost must never decrease (monotonically non-decreasing)\n",
    "df_wicket_check = df_events.withColumn(\"prev_wickets\", lag(\"wickets_lost\").over(score_window))\n",
    "decreasing_wickets = df_wicket_check.filter(\n",
    "    col(\"prev_wickets\").isNotNull() & (col(\"wickets_lost\") < col(\"prev_wickets\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"transformation\", \"ST-05: wickets_lost_monotonic\",\n",
    "    \"Running wickets count must never decrease within an innings\",\n",
    "    events_count, events_count - decreasing_wickets, 100.0,\n",
    "    f\"Wicket count decreases detected: {decreasing_wickets}\")\n",
    "\n",
    "# ST-06: Score increment between balls should equal runs column\n",
    "df_score_diff = df_score_check.withColumn(\"score_diff\",\n",
    "    col(\"innings_score\") - col(\"prev_score\"))\n",
    "mismatched_runs = df_score_diff.filter(\n",
    "    col(\"prev_score\").isNotNull() & (col(\"score_diff\") != col(\"runs\"))\n",
    ").count()\n",
    "has_prev = df_score_diff.filter(col(\"prev_score\").isNotNull()).count()\n",
    "log_dq_result(SILVER_EVENTS, \"transformation\", \"ST-06: score_increment_equals_runs\",\n",
    "    \"Score increment between consecutive balls must equal the calculated runs column\",\n",
    "    has_prev, has_prev - mismatched_runs, 100.0,\n",
    "    f\"Mismatched increments (score_diff != runs): {mismatched_runs}\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# Metadata Transformation Quality\n",
    "# ──────────────────────────────────────────────\n",
    "print(\"\\n--- Metadata Transformations ---\")\n",
    "\n",
    "# ST-07: Toss winner should match first_innings or second_innings team\n",
    "toss_not_in_teams = df_metadata.filter(\n",
    "    col(\"toss\").isNotNull() &\n",
    "    col(\"first_innings\").isNotNull() &\n",
    "    col(\"second_innings\").isNotNull() &\n",
    "    (col(\"toss\") != col(\"first_innings\")) &\n",
    "    (col(\"toss\") != col(\"second_innings\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"transformation\", \"ST-07: toss_winner_in_teams\",\n",
    "    \"Toss winner must be either the first or second innings team\",\n",
    "    metadata_count, metadata_count - toss_not_in_teams, 95.0,\n",
    "    f\"Toss winner not matching either team: {toss_not_in_teams}\")\n",
    "\n",
    "# ST-08: Session chronological: first_innings_end_utc <= second_innings_start_utc\n",
    "invalid_session_gap = df_metadata.filter(\n",
    "    col(\"first_innings_end_utc\").isNotNull() &\n",
    "    col(\"second_innings_start_utc\").isNotNull() &\n",
    "    (col(\"first_innings_end_utc\") > col(\"second_innings_start_utc\"))\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"transformation\", \"ST-08: session_chronological_order\",\n",
    "    \"First innings end UTC must be <= second innings start UTC\",\n",
    "    metadata_count, metadata_count - invalid_session_gap, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b49ae743-17e6-4b19-a6d5-236ed88be98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.6 ACCURACY / DOMAIN-SPECIFIC RULES\n",
    "_Cricket-specific business logic validation on Silver-derived data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b89e463-c384-4971-a427-cf3bf5f4a55b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: ACCURACY / DOMAIN-SPECIFIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# SA-01: Max 10 wickets per innings\n",
    "max_wickets = df_events.groupBy(\"matchid\", \"team\", \"super_over_team\").agg(\n",
    "    _max(\"wickets_lost\").alias(\"max_wickets\")\n",
    ")\n",
    "over_10_wickets = max_wickets.filter(col(\"max_wickets\") > 10).count()\n",
    "total_innings = max_wickets.count()\n",
    "log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-01: max_10_wickets_per_innings\",\n",
    "    \"No innings should have more than 10 wickets\",\n",
    "    total_innings, total_innings - over_10_wickets, 100.0)\n",
    "\n",
    "# SA-02: No bowler should bowl more than 4 overs in a T20 innings\n",
    "bowler_overs = df_events.filter(\n",
    "    (col(\"is_super_over\") == False) & col(\"bowler\").isNotNull()\n",
    ").groupBy(\"matchid\", \"team\", \"bowler\").agg(\n",
    "    countDistinct(\"over\").alias(\"overs_bowled\")\n",
    ")\n",
    "over_limit = bowler_overs.filter(col(\"overs_bowled\") > 4).count()\n",
    "total_spells = bowler_overs.count()\n",
    "log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-02: bowler_max_4_overs\",\n",
    "    \"No bowler should bowl more than 4 overs in a T20 innings\",\n",
    "    total_spells, total_spells - over_limit, 95.0,\n",
    "    f\"Bowler spells exceeding 4 overs: {over_limit}\")\n",
    "\n",
    "# SA-03: Balls with runs_text 'OUT' should have runs = 0\n",
    "total_outs = df_events.filter(col(\"runs_text\") == \"OUT\").count()\n",
    "if total_outs > 0:\n",
    "    out_with_runs = df_events.filter(\n",
    "        (col(\"runs_text\") == \"OUT\") & (col(\"runs\") != 0)\n",
    "    ).count()\n",
    "    log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-03: out_ball_zero_runs\",\n",
    "        \"Balls with runs_text='OUT' should have calculated runs=0\",\n",
    "        total_outs, total_outs - out_with_runs, 100.0)\n",
    "\n",
    "# SA-04: No-ball deliveries must have runs >= 1 (penalty run)\n",
    "total_noballs = df_events.filter(col(\"Extras\") == \"noball\").count()\n",
    "if total_noballs > 0:\n",
    "    noball_zero = df_events.filter(\n",
    "        (col(\"Extras\") == \"noball\") & (col(\"runs\") < 1)\n",
    "    ).count()\n",
    "    log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-04: noball_minimum_1_run\",\n",
    "        \"No-ball deliveries must have at least 1 run (penalty run added in Silver)\",\n",
    "        total_noballs, total_noballs - noball_zero, 100.0)\n",
    "\n",
    "# SA-05: 'FOUR runs' should calculate to 4 (or 5 with no-ball)\n",
    "total_fours = df_events.filter(col(\"runs_text\").contains(\"FOUR\")).count()\n",
    "if total_fours > 0:\n",
    "    four_wrong = df_events.filter(\n",
    "        col(\"runs_text\").contains(\"FOUR\") & ~col(\"runs\").isin([4, 5])\n",
    "    ).count()\n",
    "    log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-05: four_runs_value_check\",\n",
    "        \"FOUR runs_text should calculate to 4 (or 5 with no-ball penalty)\",\n",
    "        total_fours, total_fours - four_wrong, 100.0)\n",
    "\n",
    "# SA-06: 'SIX runs' should calculate to 6 (or 7 with no-ball)\n",
    "total_sixes = df_events.filter(col(\"runs_text\").contains(\"SIX\")).count()\n",
    "if total_sixes > 0:\n",
    "    six_wrong = df_events.filter(\n",
    "        col(\"runs_text\").contains(\"SIX\") & ~col(\"runs\").isin([6, 7])\n",
    "    ).count()\n",
    "    log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-06: six_runs_value_check\",\n",
    "        \"SIX runs_text should calculate to 6 (or 7 with no-ball penalty)\",\n",
    "        total_sixes, total_sixes - six_wrong, 100.0)\n",
    "\n",
    "# SA-07: Final innings score should be reasonable for T20 (30-300)\n",
    "final_scores = df_events.filter(col(\"is_super_over\") == False).groupBy(\n",
    "    \"matchid\", \"team\"\n",
    ").agg(_max(\"innings_score\").alias(\"final_score\"))\n",
    "unreasonable = final_scores.filter(\n",
    "    (col(\"final_score\") < 30) | (col(\"final_score\") > 300)\n",
    ").count()\n",
    "total_team_innings = final_scores.count()\n",
    "log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-07: final_score_reasonable\",\n",
    "    \"Final innings score should be between 30 and 300 for T20 (excluding Super Overs)\",\n",
    "    total_team_innings, total_team_innings - unreasonable, 90.0)\n",
    "\n",
    "# SA-08: Commentary text should have minimum length (not truncated)\n",
    "short_commentary = df_events.filter(\n",
    "    col(\"commentary\").isNotNull() & (length(trim(col(\"commentary\"))) < 5)\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"accuracy\", \"SA-08: commentary_min_length\",\n",
    "    \"Commentary text should be at least 5 characters (not truncated or garbage)\",\n",
    "    events_count, events_count - short_commentary, 75.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79b4ab31-b239-4020-a852-01ab20e2c196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.7 VOLUME / STATISTICAL RULES\n",
    "_Checks row counts, distributions, and detects anomalies in Silver data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdbb1f55-846b-4700-a254-aef3c23b8d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: VOLUME / STATISTICAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# SS-01: Tables must not be empty\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SS-01: events_table_not_empty\",\n",
    "    \"Silver match_events table must contain data\",\n",
    "    1, 1 if events_count > 0 else 0, 100.0)\n",
    "\n",
    "log_dq_result(SILVER_METADATA, \"volume\", \"SS-02: metadata_table_not_empty\",\n",
    "    \"Silver match_metadata table must contain data\",\n",
    "    1, 1 if metadata_count > 0 else 0, 100.0)\n",
    "\n",
    "log_dq_result(SILVER_PLAYERS, \"volume\", \"SS-03: players_table_not_empty\",\n",
    "    \"Silver match_players table must contain data\",\n",
    "    1, 1 if players_count > 0 else 0, 100.0)\n",
    "\n",
    "# SS-04: Events per match should be 100-500 for T20\n",
    "events_per_match = df_events.groupBy(\"matchid\").count().withColumnRenamed(\"count\", \"n\")\n",
    "valid_vol = events_per_match.filter((col(\"n\") >= 100) & (col(\"n\") <= 500)).count()\n",
    "total_matches = events_per_match.count()\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SS-04: events_per_match_reasonable\",\n",
    "    \"Each T20 match should have 100-500 ball-by-ball events (including extras)\",\n",
    "    total_matches, valid_vol, 90.0)\n",
    "\n",
    "# Stats for info\n",
    "stats = events_per_match.select(\n",
    "    _min(\"n\").alias(\"min_rows\"),\n",
    "    _max(\"n\").alias(\"max_rows\"),\n",
    "    avg(\"n\").alias(\"avg_rows\")\n",
    ").first()\n",
    "print(f\"  ℹ Events per match: min={stats['min_rows']}, max={stats['max_rows']}, avg={stats['avg_rows']:.0f}\")\n",
    "\n",
    "# SS-05: Distinct bowlers per innings should be 3-10\n",
    "bowlers_per_innings = df_events.filter(col(\"bowler\").isNotNull()) \\\n",
    "    .groupBy(\"matchid\", \"team\").agg(countDistinct(\"bowler\").alias(\"bowler_count\"))\n",
    "valid_bowler_ct = bowlers_per_innings.filter(\n",
    "    (col(\"bowler_count\") >= 3) & (col(\"bowler_count\") <= 10)\n",
    ").count()\n",
    "total_innings_b = bowlers_per_innings.count()\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SS-05: bowlers_per_innings_reasonable\",\n",
    "    \"Each innings should have 3-10 distinct bowlers (T20)\",\n",
    "    total_innings_b, valid_bowler_ct, 90.0)\n",
    "\n",
    "# SS-06: Distinct batsmen per innings should be 2-11\n",
    "batsmen_per_innings = df_events.filter(col(\"batsman\").isNotNull()) \\\n",
    "    .groupBy(\"matchid\", \"team\").agg(countDistinct(\"batsman\").alias(\"batsman_count\"))\n",
    "valid_batsman_ct = batsmen_per_innings.filter(\n",
    "    (col(\"batsman_count\") >= 2) & (col(\"batsman_count\") <= 11)\n",
    ").count()\n",
    "total_innings_bt = batsmen_per_innings.count()\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SS-06: batsmen_per_innings_reasonable\",\n",
    "    \"Each innings should have 2-11 distinct batsmen\",\n",
    "    total_innings_bt, valid_batsman_ct, 90.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9325a24b-79da-463b-8cc7-41fc0079dfd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.7b BRONZE-TO-SILVER ROW COUNT VALIDATION\n",
    "_Compares row counts between Bronze and Silver tables to detect data loss, unexpected row explosion,\n",
    "or incomplete processing during the Bronze → Silver transformation._\n",
    "\n",
    "**Expected behavior:**\n",
    "- `match_events`: Silver ≤ Bronze (dedup removes duplicates, no new rows created)\n",
    "- `match_metadata`: Silver ≤ Bronze (dedup removes duplicates)\n",
    "- `match_players`: Silver ≤ Bronze (dedup removes duplicates)\n",
    "- `matchid` coverage: Every Bronze matchid should be present in Silver (no data loss)\n",
    "- Significant row drops (>5%) indicate possible transformation bugs or filter errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fce8ad7-a046-45f6-bd62-c33141af5316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: BRONZE-TO-SILVER ROW COUNT VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# Load Bronze Tables for Comparison\n",
    "# ──────────────────────────────────────────────\n",
    "BRONZE_SCHEMA = f\"{CATALOG_NAME}.bronze\"\n",
    "BRONZE_EVENTS   = f\"{BRONZE_SCHEMA}.match_events\"\n",
    "BRONZE_METADATA = f\"{BRONZE_SCHEMA}.match_metadata\"\n",
    "BRONZE_PLAYERS  = f\"{BRONZE_SCHEMA}.match_players\"\n",
    "\n",
    "df_bronze_events   = spark.table(BRONZE_EVENTS)\n",
    "df_bronze_metadata = spark.table(BRONZE_METADATA)\n",
    "df_bronze_players  = spark.table(BRONZE_PLAYERS)\n",
    "\n",
    "bronze_events_count   = df_bronze_events.count()\n",
    "bronze_metadata_count = df_bronze_metadata.count()\n",
    "bronze_players_count  = df_bronze_players.count()\n",
    "\n",
    "print(f\"\\nBronze Tables Loaded for Comparison:\")\n",
    "print(f\"  {BRONZE_EVENTS}:   {bronze_events_count:,} rows\")\n",
    "print(f\"  {BRONZE_METADATA}: {bronze_metadata_count:,} rows\")\n",
    "print(f\"  {BRONZE_PLAYERS}:  {bronze_players_count:,} rows\")\n",
    "print(f\"\\nSilver Tables (already loaded):\")\n",
    "print(f\"  {SILVER_EVENTS}:   {events_count:,} rows\")\n",
    "print(f\"  {SILVER_METADATA}: {metadata_count:,} rows\")\n",
    "print(f\"  {SILVER_PLAYERS}:  {players_count:,} rows\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SR-01 to SR-03: Silver row count must not exceed Bronze\n",
    "# (Silver applies dedup — rows should stay same or decrease)\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- Row Count: Silver <= Bronze (no row explosion) ---\")\n",
    "\n",
    "# SR-01: match_events — Silver <= Bronze\n",
    "events_no_explosion = 1 if events_count <= bronze_events_count else 0\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SR-01: events_silver_lte_bronze\",\n",
    "    \"Silver match_events row count must not exceed Bronze (dedup should only remove rows)\",\n",
    "    1, events_no_explosion, 100.0,\n",
    "    f\"Bronze: {bronze_events_count:,}, Silver: {events_count:,}, \"\n",
    "    f\"Diff: {bronze_events_count - events_count:,} rows removed by dedup\")\n",
    "\n",
    "# SR-02: match_metadata — Silver <= Bronze\n",
    "metadata_no_explosion = 1 if metadata_count <= bronze_metadata_count else 0\n",
    "log_dq_result(SILVER_METADATA, \"volume\", \"SR-02: metadata_silver_lte_bronze\",\n",
    "    \"Silver match_metadata row count must not exceed Bronze (dedup should only remove rows)\",\n",
    "    1, metadata_no_explosion, 100.0,\n",
    "    f\"Bronze: {bronze_metadata_count:,}, Silver: {metadata_count:,}, \"\n",
    "    f\"Diff: {bronze_metadata_count - metadata_count:,} rows removed by dedup\")\n",
    "\n",
    "# SR-03: match_players — Silver <= Bronze\n",
    "players_no_explosion = 1 if players_count <= bronze_players_count else 0\n",
    "log_dq_result(SILVER_PLAYERS, \"volume\", \"SR-03: players_silver_lte_bronze\",\n",
    "    \"Silver match_players row count must not exceed Bronze (dedup should only remove rows)\",\n",
    "    1, players_no_explosion, 100.0,\n",
    "    f\"Bronze: {bronze_players_count:,}, Silver: {players_count:,}, \"\n",
    "    f\"Diff: {bronze_players_count - players_count:,} rows removed by dedup\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SR-04 to SR-06: No significant data loss (>5% drop = WARN)\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- Row Count: No Significant Data Loss (>5% drop) ---\")\n",
    "\n",
    "def check_row_count_retention(silver_table, bronze_count, silver_count, rule_id, table_label):\n",
    "    \"\"\"Check that Silver retains at least 95% of Bronze rows (allowing for dedup).\"\"\"\n",
    "    if bronze_count == 0:\n",
    "        log_dq_result(silver_table, \"volume\", f\"{rule_id}: {table_label}_no_data_loss\",\n",
    "            f\"Silver {table_label} should retain >=95% of Bronze rows (data loss check)\",\n",
    "            1, 0, 100.0, \"Bronze table is empty - cannot validate retention\")\n",
    "        return\n",
    "\n",
    "    retention_pct = (silver_count / bronze_count) * 100\n",
    "    drop_pct = 100 - retention_pct\n",
    "\n",
    "    log_dq_result(silver_table, \"volume\", f\"{rule_id}: {table_label}_no_data_loss\",\n",
    "        f\"Silver {table_label} should retain >=95% of Bronze rows (data loss check)\",\n",
    "        bronze_count, silver_count, 95.0,\n",
    "        f\"Bronze: {bronze_count:,}, Silver: {silver_count:,}, \"\n",
    "        f\"Retention: {retention_pct:.2f}%, Drop: {drop_pct:.2f}%\")\n",
    "\n",
    "# SR-04: match_events retention\n",
    "check_row_count_retention(SILVER_EVENTS, bronze_events_count, events_count,\n",
    "    \"SR-04\", \"events\")\n",
    "\n",
    "# SR-05: match_metadata retention\n",
    "check_row_count_retention(SILVER_METADATA, bronze_metadata_count, metadata_count,\n",
    "    \"SR-05\", \"metadata\")\n",
    "\n",
    "# SR-06: match_players retention\n",
    "check_row_count_retention(SILVER_PLAYERS, bronze_players_count, players_count,\n",
    "    \"SR-06\", \"players\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SR-07 to SR-09: matchid Coverage (no matches lost)\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- matchid Coverage: Bronze matchids present in Silver ---\")\n",
    "\n",
    "bronze_event_matchids    = df_bronze_events.select(\"matchid\").distinct()\n",
    "bronze_metadata_matchids = df_bronze_metadata.select(\"matchid\").distinct()\n",
    "bronze_player_matchids   = df_bronze_players.select(\"matchid\").distinct()\n",
    "\n",
    "silver_event_matchids    = df_events.select(\"matchid\").distinct()\n",
    "silver_metadata_matchids = df_metadata.select(\"matchid\").distinct()\n",
    "silver_player_matchids   = df_players.select(\"matchid\").distinct()\n",
    "\n",
    "# SR-07: Every Bronze events matchid should exist in Silver events\n",
    "bronze_evt_total = bronze_event_matchids.count()\n",
    "missing_evt_matchids = bronze_event_matchids.join(\n",
    "    silver_event_matchids, \"matchid\", \"left_anti\"\n",
    ").count()\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SR-07: events_matchid_coverage\",\n",
    "    \"Every matchid in Bronze events must be present in Silver events (no matches dropped)\",\n",
    "    bronze_evt_total, bronze_evt_total - missing_evt_matchids, 100.0,\n",
    "    f\"Bronze matchids: {bronze_evt_total}, Missing in Silver: {missing_evt_matchids}\")\n",
    "\n",
    "# SR-08: Every Bronze metadata matchid should exist in Silver metadata\n",
    "bronze_meta_total = bronze_metadata_matchids.count()\n",
    "missing_meta_matchids = bronze_metadata_matchids.join(\n",
    "    silver_metadata_matchids, \"matchid\", \"left_anti\"\n",
    ").count()\n",
    "log_dq_result(SILVER_METADATA, \"volume\", \"SR-08: metadata_matchid_coverage\",\n",
    "    \"Every matchid in Bronze metadata must be present in Silver metadata (no matches dropped)\",\n",
    "    bronze_meta_total, bronze_meta_total - missing_meta_matchids, 100.0,\n",
    "    f\"Bronze matchids: {bronze_meta_total}, Missing in Silver: {missing_meta_matchids}\")\n",
    "\n",
    "# SR-09: Every Bronze players matchid should exist in Silver players\n",
    "bronze_plr_total = bronze_player_matchids.count()\n",
    "missing_plr_matchids = bronze_player_matchids.join(\n",
    "    silver_player_matchids, \"matchid\", \"left_anti\"\n",
    ").count()\n",
    "log_dq_result(SILVER_PLAYERS, \"volume\", \"SR-09: players_matchid_coverage\",\n",
    "    \"Every matchid in Bronze players must be present in Silver players (no matches dropped)\",\n",
    "    bronze_plr_total, bronze_plr_total - missing_plr_matchids, 100.0,\n",
    "    f\"Bronze matchids: {bronze_plr_total}, Missing in Silver: {missing_plr_matchids}\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# SR-10 & SR-11: Per-match row count comparison (events)\n",
    "# Detect individual matches with abnormal row loss or explosion\n",
    "# ──────────────────────────────────────────────\n",
    "print(f\"\\n--- Per-Match Row Count Comparison (Events) ---\")\n",
    "\n",
    "bronze_per_match = df_bronze_events.groupBy(\"matchid\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"bronze_rows\")\n",
    "silver_per_match = df_events.groupBy(\"matchid\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"silver_rows\")\n",
    "\n",
    "row_comparison = bronze_per_match.join(silver_per_match, \"matchid\", \"left\") \\\n",
    "    .withColumn(\"silver_rows\", coalesce(col(\"silver_rows\"), lit(0))) \\\n",
    "    .withColumn(\"row_diff\", col(\"bronze_rows\") - col(\"silver_rows\")) \\\n",
    "    .withColumn(\"retention_pct\",\n",
    "        when(col(\"bronze_rows\") > 0,\n",
    "             (col(\"silver_rows\") / col(\"bronze_rows\") * 100))\n",
    "        .otherwise(0.0))\n",
    "\n",
    "# SR-10: Matches where Silver has >10% fewer rows than Bronze (abnormal loss)\n",
    "abnormal_loss = row_comparison.filter(col(\"retention_pct\") < 90).count()\n",
    "total_compared = row_comparison.count()\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SR-10: per_match_row_retention\",\n",
    "    \"Each match should retain >=90% of Bronze event rows in Silver (per-match check)\",\n",
    "    total_compared, total_compared - abnormal_loss, 95.0,\n",
    "    f\"Matches with >10% row loss: {abnormal_loss}\")\n",
    "\n",
    "# SR-11: Matches where Silver has MORE rows than Bronze (should never happen)\n",
    "row_explosion_matches = row_comparison.filter(col(\"silver_rows\") > col(\"bronze_rows\")).count()\n",
    "log_dq_result(SILVER_EVENTS, \"volume\", \"SR-11: no_per_match_row_explosion\",\n",
    "    \"No individual match should have more Silver rows than Bronze rows\",\n",
    "    total_compared, total_compared - row_explosion_matches, 100.0,\n",
    "    f\"Matches with row explosion (Silver > Bronze): {row_explosion_matches}\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# Summary Statistics\n",
    "# ──────────────────────────────────────────────\n",
    "evt_ret = (events_count / bronze_events_count * 100) if bronze_events_count > 0 else 0\n",
    "meta_ret = (metadata_count / bronze_metadata_count * 100) if bronze_metadata_count > 0 else 0\n",
    "plr_ret = (players_count / bronze_players_count * 100) if bronze_players_count > 0 else 0\n",
    "\n",
    "print(f\"\\n  ℹ Row Count Summary:\")\n",
    "print(f\"    Events   — Bronze: {bronze_events_count:,}  → Silver: {events_count:,}  \"\n",
    "      f\"(Δ {bronze_events_count - events_count:,}, {evt_ret:.2f}% retained)\")\n",
    "print(f\"    Metadata — Bronze: {bronze_metadata_count:,}  → Silver: {metadata_count:,}  \"\n",
    "      f\"(Δ {bronze_metadata_count - metadata_count:,}, {meta_ret:.2f}% retained)\")\n",
    "print(f\"    Players  — Bronze: {bronze_players_count:,}  → Silver: {players_count:,}  \"\n",
    "      f\"(Δ {bronze_players_count - players_count:,}, {plr_ret:.2f}% retained)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb6234da-2ea0-4567-a783-8bee036dfd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.8 SCHEMA VALIDATION\n",
    "_Verifies expected Silver columns are present after transformations. Unlike Bronze schema drift\n",
    "(Auto Loader), Silver schema changes indicate transformation code bugs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "754636b0-0bfb-4a40-a194-82bc12c66096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CATEGORY: SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# SD-01: Silver events expected columns (derived columns from transformations)\n",
    "expected_event_cols = {\n",
    "    \"Batchid\", \"matchid\", \"event\", \"ball\", \"over\", \"over_ball_number\",\n",
    "    \"runs\", \"innings_score\", \"wickets_lost\", \"commentary\",\n",
    "    \"bowler\", \"batsman\", \"team\", \"is_super_over\", \"super_over_team\",\n",
    "    \"runs_text\", \"Extras\", \"dismissal_method\", \"dismissed_by\",\n",
    "    \"source_file\", \"load_timestamp\", \"silver_load_timestamp\"\n",
    "}\n",
    "actual_event_cols = set(df_events.columns)\n",
    "missing_e = expected_event_cols - actual_event_cols\n",
    "extra_e = actual_event_cols - expected_event_cols\n",
    "log_dq_result(SILVER_EVENTS, \"schema\", \"SD-01: events_expected_columns\",\n",
    "    \"Silver events must contain all expected columns from transformation notebook\",\n",
    "    len(expected_event_cols), len(expected_event_cols) - len(missing_e), 100.0,\n",
    "    f\"Missing: {missing_e or 'None'}, Extra: {extra_e or 'None'}\")\n",
    "\n",
    "# SD-02: Silver metadata expected columns\n",
    "expected_meta_cols = {\n",
    "    \"Batchid\", \"matchid\", \"ground\", \"toss\", \"decision\", \"series\", \"season\",\n",
    "    \"player_of_the_match\", \"player_of_the_series\", \"t20_debut\", \"t20i_debut\",\n",
    "    \"umpires\", \"tv_umpire\", \"reserve_umpire\", \"match_referee\",\n",
    "    \"points\", \"player_replacements\", \"match_number\",\n",
    "    \"match_date\", \"match_start_utc\",\n",
    "    \"first_innings_start_utc\", \"first_innings_end_utc\",\n",
    "    \"second_innings_start_utc\", \"second_innings_end_utc\",\n",
    "    \"first_innings\", \"second_innings\",\n",
    "    \"has_super_over\", \"super_over_count\", \"series_result\",\n",
    "    \"source_file\", \"load_timestamp\", \"silver_load_timestamp\"\n",
    "}\n",
    "actual_meta_cols = set(df_metadata.columns)\n",
    "missing_m = expected_meta_cols - actual_meta_cols\n",
    "extra_m = actual_meta_cols - expected_meta_cols\n",
    "log_dq_result(SILVER_METADATA, \"schema\", \"SD-02: metadata_expected_columns\",\n",
    "    \"Silver metadata must contain all expected columns from transformation notebook\",\n",
    "    len(expected_meta_cols), len(expected_meta_cols) - len(missing_m), 100.0,\n",
    "    f\"Missing: {missing_m or 'None'}, Extra: {extra_m or 'None'}\")\n",
    "\n",
    "# SD-03: Silver players expected columns\n",
    "expected_player_cols = {\n",
    "    \"Batchid\", \"matchid\", \"innings\", \"team\", \"player_name\",\n",
    "    \"batted\", \"batting_position\", \"player_type\", \"retired\",\n",
    "    \"not_out\", \"bowled\", \"source_file\", \"load_timestamp\",\n",
    "    \"silver_load_timestamp\"\n",
    "}\n",
    "actual_player_cols = set(df_players.columns)\n",
    "missing_p = expected_player_cols - actual_player_cols\n",
    "extra_p = actual_player_cols - expected_player_cols\n",
    "log_dq_result(SILVER_PLAYERS, \"schema\", \"SD-03: players_expected_columns\",\n",
    "    \"Silver players must contain all expected columns from transformation notebook\",\n",
    "    len(expected_player_cols), len(expected_player_cols) - len(missing_p), 100.0,\n",
    "    f\"Missing: {missing_p or 'None'}, Extra: {extra_p or 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f1aaf01-cde1-4cd3-929e-47da1a3a5c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. DQ Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44818e04-e91a-4a5e-9265-f4c550b5eefa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"DATA QUALITY SUMMARY — Silver Layer — Run ID: {run_id}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall status breakdown\n",
    "summary_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        status,\n",
    "        COUNT(*) AS rule_count,\n",
    "        ROUND(AVG(pass_percentage), 2) AS avg_pass_pct\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}'\n",
    "    GROUP BY status\n",
    "    ORDER BY status\n",
    "\"\"\")\n",
    "display(summary_df)\n",
    "\n",
    "# Detailed failures\n",
    "print(\"\\n--- FAILED RULES ---\")\n",
    "failures_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        table_name, rule_category, rule_name,\n",
    "        pass_percentage, failed_records, details\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}' AND status = 'FAIL'\n",
    "    ORDER BY pass_percentage ASC\n",
    "\"\"\")\n",
    "display(failures_df)\n",
    "\n",
    "# Warnings\n",
    "print(\"\\n--- WARNINGS ---\")\n",
    "warnings_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        table_name, rule_category, rule_name,\n",
    "        pass_percentage, failed_records, details\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}' AND status = 'WARN'\n",
    "    ORDER BY pass_percentage ASC\n",
    "\"\"\")\n",
    "display(warnings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6fdeac6-74cb-4767-8771-7ef06e40ecfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Historical DQ Trend (Optional Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba073d48-38a2-496e-bda5-6d802369cb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare Silver DQ pass rates over time\n",
    "trend_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_id,\n",
    "        run_timestamp,\n",
    "        COUNT(*) AS total_rules,\n",
    "        SUM(CASE WHEN status = 'PASS' THEN 1 ELSE 0 END) AS passed,\n",
    "        SUM(CASE WHEN status = 'WARN' THEN 1 ELSE 0 END) AS warnings,\n",
    "        SUM(CASE WHEN status = 'FAIL' THEN 1 ELSE 0 END) AS failed,\n",
    "        ROUND(AVG(pass_percentage), 2) AS avg_pass_pct\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id LIKE 'SLV_%'\n",
    "    GROUP BY run_id, run_timestamp\n",
    "    ORDER BY run_timestamp DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "display(trend_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d8e377-8fb5-4d72-a9eb-cbc1773a2086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Pipeline Gate (Optional — Fail pipeline if critical rules fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "306b2f11-e44d-4ec7-8655-9c0cb9601d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for critical failures and optionally halt downstream processing\n",
    "critical_failures = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS cnt\n",
    "    FROM {DQ_AUDIT_TABLE}\n",
    "    WHERE run_id = '{run_id}'\n",
    "      AND status = 'FAIL'\n",
    "      AND threshold_pct = 100.0\n",
    "\"\"\").first()[\"cnt\"]\n",
    "\n",
    "if critical_failures > 0:\n",
    "    msg = f\"⛔ PIPELINE GATE: {critical_failures} critical Silver DQ rule(s) failed! Review before proceeding to Gold layer.\"\n",
    "    print(msg)\n",
    "    # Uncomment to actually halt the pipeline:\n",
    "    # raise Exception(msg)\n",
    "else:\n",
    "    print(\"✅ PIPELINE GATE: All critical Silver DQ rules passed. Safe to proceed to Gold layer.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8598518854948756,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DataQualityRulesSilverLayer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
